2018-04-24 14:12:12,777 (dqn_main.py:79) DEBUG: ">>>> RUNNING data_file=5_15.npy,step_reward=-0.1,fast_fail=True,reuse_weights=True,test=False<<<<"
2018-04-24 14:12:13,176 (tf_logging.py:160) Level 1: "Registering Batch (<function _BatchGrad at 0x7f89453739d8>) in gradient."
2018-04-24 14:12:13,178 (tf_logging.py:160) Level 1: "Registering Unbatch (<function _UnbatchGrad at 0x7f8945325510>) in gradient."
2018-04-24 14:12:13,183 (tf_logging.py:160) Level 1: "Registering ZeroInitializer (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,204 (tf_logging.py:160) Level 1: "Registering SparseFeatureCross (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,207 (tf_logging.py:160) Level 1: "Registering SparseFeatureCrossV2 (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,214 (tf_logging.py:160) Level 1: "Registering SparseFeatureCross (None) in gradient."
2018-04-24 14:12:13,216 (tf_logging.py:160) Level 1: "Registering SparseFeatureCrossV2 (None) in gradient."
2018-04-24 14:12:13,224 (tf_logging.py:160) Level 1: "Registering GDNLowerBound (<function GDN._lower_bound_grad at 0x7f8944258d08>) in gradient."
2018-04-24 14:12:13,250 (tf_logging.py:160) Level 1: "Registering ObtainNext (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,263 (tf_logging.py:160) Level 1: "Registering hparams ((<class 'tensorflow.contrib.training.python.training.hparam_pb2.HParamDef'>, <function HParams.to_proto at 0x7f8944051c80>, <function HParams.from_proto at 0x7f8944051d08>)) in proto functions."
2018-04-24 14:12:13,275 (tf_logging.py:160) Level 1: "Registering GRUBlockCell (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,280 (tf_logging.py:160) Level 1: "Registering GRUBlockCellGrad (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,284 (tf_logging.py:160) Level 1: "Registering GRUBlockCell (<function _GRUBlockCellGrad at 0x7f892cfad268>) in gradient."
2018-04-24 14:12:13,288 (tf_logging.py:160) Level 1: "Registering BlockLSTM (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,292 (tf_logging.py:160) Level 1: "Registering BlockLSTMGrad (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,295 (tf_logging.py:160) Level 1: "Registering LSTMBlockCell (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,299 (tf_logging.py:160) Level 1: "Registering LSTMBlockCellGrad (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,303 (tf_logging.py:160) Level 1: "Registering LSTMBlockCell (<function _LSTMBlockCellGrad at 0x7f892cf69488>) in gradient."
2018-04-24 14:12:13,304 (tf_logging.py:160) Level 1: "Registering BlockLSTM (<function _BlockLSTMGrad at 0x7f892cf69510>) in gradient."
2018-04-24 14:12:13,310 (tf_logging.py:160) Level 1: "Registering KMC2ChainInitialization (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,311 (tf_logging.py:160) Level 1: "Registering KmeansPlusPlusInitialization (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,312 (tf_logging.py:160) Level 1: "Registering NearestNeighbors (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,316 (tf_logging.py:160) Level 1: "Registering MaskedMatmul (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,318 (tf_logging.py:160) Level 1: "Registering WALSComputePartialLhsAndRhs (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,335 (tf_logging.py:160) Level 1: "Registering ConfigureDistributedTPU (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,337 (tf_logging.py:160) Level 1: "Registering CrossReplicaSum (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,338 (tf_logging.py:160) Level 1: "Registering InfeedDequeue (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,338 (tf_logging.py:160) Level 1: "Registering InfeedDequeueTuple (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,339 (tf_logging.py:160) Level 1: "Registering InfeedEnqueue (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,339 (tf_logging.py:160) Level 1: "Registering InfeedEnqueueTuple (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,339 (tf_logging.py:160) Level 1: "Registering OutfeedDequeue (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,340 (tf_logging.py:160) Level 1: "Registering OutfeedDequeueTuple (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,340 (tf_logging.py:160) Level 1: "Registering OutfeedEnqueue (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,341 (tf_logging.py:160) Level 1: "Registering OutfeedEnqueueTuple (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,342 (tf_logging.py:160) Level 1: "Registering ShutdownDistributedTPU (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,342 (tf_logging.py:160) Level 1: "Registering TPUEmbeddingActivations (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,343 (tf_logging.py:160) Level 1: "Registering TPUEmbeddingEnqueueSparseBatch (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,344 (tf_logging.py:160) Level 1: "Registering TPUEmbeddingLoadAdagradParameters (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,345 (tf_logging.py:160) Level 1: "Registering TPUEmbeddingLoadGradientDescentParameters (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,345 (tf_logging.py:160) Level 1: "Registering TPUEmbeddingReceiveActivations (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,346 (tf_logging.py:160) Level 1: "Registering TPUEmbeddingRetrieveAdagradParameters (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,347 (tf_logging.py:160) Level 1: "Registering TPUEmbeddingRetrieveGradientDescentParameters (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,348 (tf_logging.py:160) Level 1: "Registering TPUEmbeddingSendGradients (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,348 (tf_logging.py:160) Level 1: "Registering TPUReplicate (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,349 (tf_logging.py:160) Level 1: "Registering TPUReplicateMetadata (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,350 (tf_logging.py:160) Level 1: "Registering TPUReplicatedInput (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,351 (tf_logging.py:160) Level 1: "Registering TPUReplicatedOutput (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,358 (tf_logging.py:160) Level 1: "Registering _ConfigureDistributedTPU (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,359 (tf_logging.py:160) Level 1: "Registering _WaitForDistributedTPU (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,360 (tf_logging.py:160) Level 1: "Registering _SetGlobalTPUArray (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,361 (tf_logging.py:160) Level 1: "Registering _ShutdownDistributedTPU (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,362 (tf_logging.py:160) Level 1: "Registering _InitializeHostForDistributedTPU (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,362 (tf_logging.py:160) Level 1: "Registering _DisconnectHostFromDistributedTPUSystem (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,365 (tf_logging.py:160) Level 1: "Registering CrossReplicaSum (<function _cross_replica_sum_grad at 0x7f892c153a60>) in gradient."
2018-04-24 14:12:13,370 (tf_logging.py:160) Level 1: "Registering CloseSummaryWriter (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,371 (tf_logging.py:160) Level 1: "Registering CreateSummaryDbWriter (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,371 (tf_logging.py:160) Level 1: "Registering CreateSummaryFileWriter (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,372 (tf_logging.py:160) Level 1: "Registering FlushSummaryWriter (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,372 (tf_logging.py:160) Level 1: "Registering ImportEvent (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,373 (tf_logging.py:160) Level 1: "Registering SummaryWriter (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,373 (tf_logging.py:160) Level 1: "Registering WriteAudioSummary (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,374 (tf_logging.py:160) Level 1: "Registering WriteGraphSummary (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,374 (tf_logging.py:160) Level 1: "Registering WriteHistogramSummary (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,374 (tf_logging.py:160) Level 1: "Registering WriteImageSummary (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,375 (tf_logging.py:160) Level 1: "Registering WriteScalarSummary (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,375 (tf_logging.py:160) Level 1: "Registering WriteSummary (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,393 (tf_logging.py:160) Level 1: "Registering BigQueryReader (None) in gradient."
2018-04-24 14:12:13,396 (tf_logging.py:160) Level 1: "Registering RangeDecode (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,396 (tf_logging.py:160) Level 1: "Registering RangeEncode (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,447 (tf_logging.py:160) Level 1: "Registering CudnnRNN (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,448 (tf_logging.py:160) Level 1: "Registering CudnnRNNBackprop (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,448 (tf_logging.py:160) Level 1: "Registering CudnnRNNCanonicalToParams (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,449 (tf_logging.py:160) Level 1: "Registering CudnnRNNParamsSize (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,449 (tf_logging.py:160) Level 1: "Registering CudnnRNNParamsToCanonical (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,454 (tf_logging.py:160) Level 1: "Registering CudnnRNN (<function _cudnn_rnn_backward at 0x7f8918168d08>) in gradient."
2018-04-24 14:12:13,455 (tf_logging.py:160) Level 1: "Registering CudnnRNNParamsSize (<function call_cpp_shape_fn at 0x7f89b0ada950>) in shape functions."
2018-04-24 14:12:13,455 (tf_logging.py:160) Level 1: "Registering CudnnRNNParamsToCanonical (<function call_cpp_shape_fn at 0x7f89b0ada950>) in shape functions."
2018-04-24 14:12:13,455 (tf_logging.py:160) Level 1: "Registering CudnnRNNCanonicalToParams (<function call_cpp_shape_fn at 0x7f89b0ada950>) in shape functions."
2018-04-24 14:12:13,455 (tf_logging.py:160) Level 1: "Registering CudnnRNN (<function call_cpp_shape_fn at 0x7f89b0ada950>) in shape functions."
2018-04-24 14:12:13,455 (tf_logging.py:160) Level 1: "Registering CudnnRNNBackprop (<function call_cpp_shape_fn at 0x7f89b0ada950>) in shape functions."
2018-04-24 14:12:13,500 (tf_logging.py:160) Level 1: "Registering AdjustHsvInYiq (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,503 (tf_logging.py:160) Level 1: "Registering BipartiteMatch (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,504 (tf_logging.py:160) Level 1: "Registering ImageConnectedComponents (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,504 (tf_logging.py:160) Level 1: "Registering ImageProjectiveTransform (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,518 (tf_logging.py:160) Level 1: "Registering ImageConnectedComponents (<function call_cpp_shape_fn at 0x7f89b0ada950>) in shape functions."
2018-04-24 14:12:13,518 (tf_logging.py:160) Level 1: "Registering ImageProjectiveTransform (<function call_cpp_shape_fn at 0x7f89b0ada950>) in shape functions."
2018-04-24 14:12:13,518 (tf_logging.py:160) Level 1: "Registering ImageProjectiveTransform (<function _image_projective_transform_grad at 0x7f89100f7268>) in gradient."
2018-04-24 14:12:13,519 (tf_logging.py:160) Level 1: "Registering BipartiteMatch (None) in gradient."
2018-04-24 14:12:13,519 (tf_logging.py:160) Level 1: "Registering ImageConnectedComponents (None) in gradient."
2018-04-24 14:12:13,519 (tf_logging.py:160) Level 1: "Registering SingleImageRandomDotStereograms (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,525 (tf_logging.py:160) Level 1: "Registering SingleImageRandomDotStereograms (None) in gradient."
2018-04-24 14:12:13,601 (tf_logging.py:160) Level 1: "Registering BytesInUse (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,603 (tf_logging.py:160) Level 1: "Registering BytesLimit (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,604 (tf_logging.py:160) Level 1: "Registering MaxBytesInUse (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,611 (tf_logging.py:160) Level 1: "Registering NcclAllReduce (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,618 (tf_logging.py:160) Level 1: "Registering NcclBroadcast (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,619 (tf_logging.py:160) Level 1: "Registering NcclReduce (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,623 (tf_logging.py:160) Level 1: "Registering _NcclReduceSend (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,626 (tf_logging.py:160) Level 1: "Registering _NcclReduceRecv (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,627 (tf_logging.py:160) Level 1: "Registering _NcclBroadcastSend (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,628 (tf_logging.py:160) Level 1: "Registering _NcclBroadcastRecv (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,630 (tf_logging.py:160) Level 1: "Registering NcclAllReduce (<function _all_sum_grad at 0x7f88ee6c5048>) in gradient."
2018-04-24 14:12:13,631 (tf_logging.py:160) Level 1: "Registering NcclReduce (<function _reduce_sum_grad at 0x7f88ee6c52f0>) in gradient."
2018-04-24 14:12:13,632 (tf_logging.py:160) Level 1: "Registering NcclBroadcast (<function _broadcast_grad at 0x7f88ee6c5488>) in gradient."
2018-04-24 14:12:13,634 (tf_logging.py:160) Level 1: "Registering PeriodicResample (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,642 (tf_logging.py:160) Level 1: "Registering FoldFusedBatchNormGrad (<function _FoldFusedBatchNormGrad at 0x7f88ed0e26a8>) in gradient."
2018-04-24 14:12:13,644 (tf_logging.py:160) Level 1: "Registering Resampler (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,645 (tf_logging.py:160) Level 1: "Registering ResamplerGrad (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,647 (tf_logging.py:160) Level 1: "Registering Resampler (<function _resampler_grad at 0x7f88ece29378>) in gradient."
2018-04-24 14:12:13,648 (tf_logging.py:160) Level 1: "Registering ResamplerGrad (None) in gradient."
2018-04-24 14:12:13,652 (tf_logging.py:160) Level 1: "Registering GatherTree (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,659 (tf_logging.py:160) Level 1: "Registering StatelessRandomNormal (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,660 (tf_logging.py:160) Level 1: "Registering StatelessRandomUniform (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,660 (tf_logging.py:160) Level 1: "Registering StatelessTruncatedNormal (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,661 (tf_logging.py:160) Level 1: "Registering StatelessRandomNormal (None) in gradient."
2018-04-24 14:12:13,661 (tf_logging.py:160) Level 1: "Registering StatelessRandomUniform (None) in gradient."
2018-04-24 14:12:13,661 (tf_logging.py:160) Level 1: "Registering StatelessTruncatedNormal (None) in gradient."
2018-04-24 14:12:13,666 (tf_logging.py:160) Level 1: "Registering ReinterpretStringToFloat (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,667 (tf_logging.py:160) Level 1: "Registering ScatterAddNdim (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,672 (tf_logging.py:160) Level 1: "Registering CreateTreeVariable (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,673 (tf_logging.py:160) Level 1: "Registering DecisionTreeResourceHandleOp (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,674 (tf_logging.py:160) Level 1: "Registering FeatureUsageCounts (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,674 (tf_logging.py:160) Level 1: "Registering TraverseTreeV4 (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,674 (tf_logging.py:160) Level 1: "Registering TreeDeserialize (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,674 (tf_logging.py:160) Level 1: "Registering TreeIsInitializedOp (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,675 (tf_logging.py:160) Level 1: "Registering TreePredictionsV4 (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,676 (tf_logging.py:160) Level 1: "Registering TreeSerialize (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,676 (tf_logging.py:160) Level 1: "Registering TreeSize (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,676 (tf_logging.py:160) Level 1: "Registering UpdateModelV4 (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,679 (tf_logging.py:160) Level 1: "Registering TreeVariable (None) in gradient."
2018-04-24 14:12:13,680 (tf_logging.py:160) Level 1: "Registering TreeSerialize (None) in gradient."
2018-04-24 14:12:13,680 (tf_logging.py:160) Level 1: "Registering TreeDeserialize (None) in gradient."
2018-04-24 14:12:13,680 (tf_logging.py:160) Level 1: "Registering TreeSize (None) in gradient."
2018-04-24 14:12:13,680 (tf_logging.py:160) Level 1: "Registering TreePredictionsV4 (None) in gradient."
2018-04-24 14:12:13,681 (tf_logging.py:160) Level 1: "Registering FeatureUsageCounts (None) in gradient."
2018-04-24 14:12:13,681 (tf_logging.py:160) Level 1: "Registering CreateFertileStatsVariable (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,682 (tf_logging.py:160) Level 1: "Registering FertileStatsDeserialize (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,682 (tf_logging.py:160) Level 1: "Registering FertileStatsIsInitializedOp (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,683 (tf_logging.py:160) Level 1: "Registering FertileStatsResourceHandleOp (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,683 (tf_logging.py:160) Level 1: "Registering FertileStatsSerialize (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,683 (tf_logging.py:160) Level 1: "Registering FinalizeTree (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,683 (tf_logging.py:160) Level 1: "Registering GrowTreeV4 (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,684 (tf_logging.py:160) Level 1: "Registering ProcessInputV4 (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,686 (tf_logging.py:160) Level 1: "Registering FertileStatsVariable (None) in gradient."
2018-04-24 14:12:13,687 (tf_logging.py:160) Level 1: "Registering FertileStatsSerialize (None) in gradient."
2018-04-24 14:12:13,687 (tf_logging.py:160) Level 1: "Registering FertileStatsDeserialize (None) in gradient."
2018-04-24 14:12:13,687 (tf_logging.py:160) Level 1: "Registering GrowTreeV4 (None) in gradient."
2018-04-24 14:12:13,687 (tf_logging.py:160) Level 1: "Registering ProcessInputV4 (None) in gradient."
2018-04-24 14:12:13,688 (tf_logging.py:160) Level 1: "Registering FinalizeTree (None) in gradient."
2018-04-24 14:12:13,702 (tf_logging.py:160) Level 1: "Registering FunctionBufferingResource (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,702 (tf_logging.py:160) Level 1: "Registering FunctionBufferingResourceGetNext (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,710 (tf_logging.py:160) Level 1: "Registering RemoteFusedGraphExecute (<function _set_call_cpp_shape_fn.<locals>.call_without_requiring at 0x7f89b0adaa60>) in default shape functions."
2018-04-24 14:12:13,711 (tf_logging.py:160) Level 1: "Registering RemoteFusedGraphExecute (None) in gradient."
2018-04-24 14:12:13,719 (tf_logging.py:160) Level 1: "Created variable q/Conv/weights:0 with shape (1, 1, 2, 8) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944256b70>"
2018-04-24 14:12:13,722 (tf_logging.py:160) Level 1: "Created variable q/Conv/biases:0 with shape (8,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f89442674a8>"
2018-04-24 14:12:13,729 (tf_logging.py:160) Level 1: "Created variable q/Conv_1/weights:0 with shape (3, 3, 8, 12) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944256b70>"
2018-04-24 14:12:13,732 (tf_logging.py:160) Level 1: "Created variable q/Conv_1/biases:0 with shape (12,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f89442674a8>"
2018-04-24 14:12:13,739 (tf_logging.py:160) Level 1: "Created variable q/Conv_2/weights:0 with shape (3, 3, 12, 16) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944256b70>"
2018-04-24 14:12:13,743 (tf_logging.py:160) Level 1: "Created variable q/Conv_2/biases:0 with shape (16,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f89442674a8>"
2018-04-24 14:12:13,755 (tf_logging.py:160) Level 1: "Created variable q/fully_connected/weights:0 with shape (576, 128) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944258950>"
2018-04-24 14:12:13,759 (tf_logging.py:160) Level 1: "Created variable q/fully_connected/biases:0 with shape (128,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f8944267898>"
2018-04-24 14:12:13,766 (tf_logging.py:160) Level 1: "Created variable q/fully_connected_1/weights:0 with shape (128, 5) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944258950>"
2018-04-24 14:12:13,770 (tf_logging.py:160) Level 1: "Created variable q/fully_connected_1/biases:0 with shape (5,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f8944267898>"
2018-04-24 14:12:13,778 (tf_logging.py:160) Level 1: "Created variable target_q/Conv/weights:0 with shape (1, 1, 2, 8) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944256b70>"
2018-04-24 14:12:13,782 (tf_logging.py:160) Level 1: "Created variable target_q/Conv/biases:0 with shape (8,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f89442674a8>"
2018-04-24 14:12:13,791 (tf_logging.py:160) Level 1: "Created variable target_q/Conv_1/weights:0 with shape (3, 3, 8, 12) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944256b70>"
2018-04-24 14:12:13,795 (tf_logging.py:160) Level 1: "Created variable target_q/Conv_1/biases:0 with shape (12,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f89442674a8>"
2018-04-24 14:12:13,803 (tf_logging.py:160) Level 1: "Created variable target_q/Conv_2/weights:0 with shape (3, 3, 12, 16) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944256b70>"
2018-04-24 14:12:13,807 (tf_logging.py:160) Level 1: "Created variable target_q/Conv_2/biases:0 with shape (16,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f89442674a8>"
2018-04-24 14:12:13,821 (tf_logging.py:160) Level 1: "Created variable target_q/fully_connected/weights:0 with shape (576, 128) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944258950>"
2018-04-24 14:12:13,825 (tf_logging.py:160) Level 1: "Created variable target_q/fully_connected/biases:0 with shape (128,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f8944267898>"
2018-04-24 14:12:13,832 (tf_logging.py:160) Level 1: "Created variable target_q/fully_connected_1/weights:0 with shape (128, 5) and init <function variance_scaling_initializer.<locals>._initializer at 0x7f8944258950>"
2018-04-24 14:12:13,836 (tf_logging.py:160) Level 1: "Created variable target_q/fully_connected_1/biases:0 with shape (5,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f8944267898>"
2018-04-24 14:12:13,876 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/value'"
2018-04-24 14:12:13,877 (tf_logging.py:160) Level 1: "  in  --> gradients/Fill:0"
2018-04-24 14:12:13,877 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/value_grad/tuple/control_dependency:0, gradients/mean_squared_error/value_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,886 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/div'"
2018-04-24 14:12:13,887 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/value_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,887 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/div_grad/tuple/control_dependency:0, gradients/mean_squared_error/div_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,890 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/Sum_1'"
2018-04-24 14:12:13,890 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/div_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,890 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/Sum_1_grad/Tile:0"
2018-04-24 14:12:13,894 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/Select'"
2018-04-24 14:12:13,894 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/div_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,894 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/Select_grad/tuple/control_dependency:0, gradients/mean_squared_error/Select_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,897 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/Sum'"
2018-04-24 14:12:13,897 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/Sum_1_grad/Tile:0"
2018-04-24 14:12:13,897 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/Sum_grad/Tile:0"
2018-04-24 14:12:13,906 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/Mul'"
2018-04-24 14:12:13,907 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/Sum_grad/Tile:0"
2018-04-24 14:12:13,907 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/Mul_grad/tuple/control_dependency:0, gradients/mean_squared_error/Mul_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,910 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/num_present'"
2018-04-24 14:12:13,914 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/Select_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,914 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/num_present_grad/Tile:0"
2018-04-24 14:12:13,921 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/num_present/broadcast_weights'"
2018-04-24 14:12:13,928 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/num_present_grad/Tile:0"
2018-04-24 14:12:13,928 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency:0, gradients/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,930 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/num_present/broadcast_weights/ones_like'"
2018-04-24 14:12:13,932 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/num_present/broadcast_weights_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,933 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/num_present/broadcast_weights/ones_like_grad/Sum:0"
2018-04-24 14:12:13,941 (tf_logging.py:160) Level 1: "Gradient for 'mean_squared_error/SquaredDifference'"
2018-04-24 14:12:13,941 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/Mul_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,942 (tf_logging.py:160) Level 1: "  out --> gradients/mean_squared_error/SquaredDifference_grad/tuple/control_dependency:0, gradients/mean_squared_error/SquaredDifference_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,949 (tf_logging.py:160) Level 1: "Gradient for 'Sum'"
2018-04-24 14:12:13,949 (tf_logging.py:160) Level 1: "  in  --> gradients/mean_squared_error/SquaredDifference_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,949 (tf_logging.py:160) Level 1: "  out --> gradients/Sum_grad/Tile:0"
2018-04-24 14:12:13,954 (tf_logging.py:160) Level 1: "Gradient for 'mul'"
2018-04-24 14:12:13,955 (tf_logging.py:160) Level 1: "  in  --> gradients/Sum_grad/Tile:0"
2018-04-24 14:12:13,955 (tf_logging.py:160) Level 1: "  out --> gradients/mul_grad/tuple/control_dependency:0, gradients/mul_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,957 (tf_logging.py:160) Level 1: "Gradient for 'q/fully_connected_1/BiasAdd'"
2018-04-24 14:12:13,957 (tf_logging.py:160) Level 1: "  in  --> gradients/mul_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,957 (tf_logging.py:160) Level 1: "  out --> gradients/q/fully_connected_1/BiasAdd_grad/tuple/control_dependency:0, gradients/q/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,959 (tf_logging.py:160) Level 1: "Gradient for 'q/fully_connected_1/MatMul'"
2018-04-24 14:12:13,959 (tf_logging.py:160) Level 1: "  in  --> gradients/q/fully_connected_1/BiasAdd_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,959 (tf_logging.py:160) Level 1: "  out --> gradients/q/fully_connected_1/MatMul_grad/tuple/control_dependency:0, gradients/q/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,960 (tf_logging.py:160) Level 1: "Gradient for 'q/fully_connected_1/biases/read'"
2018-04-24 14:12:13,960 (tf_logging.py:160) Level 1: "  in  --> gradients/q/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,960 (tf_logging.py:160) Level 1: "  out --> gradients/q/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,961 (tf_logging.py:160) Level 1: "Gradient for 'q/fully_connected/Relu'"
2018-04-24 14:12:13,961 (tf_logging.py:160) Level 1: "  in  --> gradients/q/fully_connected_1/MatMul_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,961 (tf_logging.py:160) Level 1: "  out --> gradients/q/fully_connected/Relu_grad/ReluGrad:0"
2018-04-24 14:12:13,961 (tf_logging.py:160) Level 1: "Gradient for 'q/fully_connected_1/weights/read'"
2018-04-24 14:12:13,961 (tf_logging.py:160) Level 1: "  in  --> gradients/q/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,961 (tf_logging.py:160) Level 1: "  out --> gradients/q/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,963 (tf_logging.py:160) Level 1: "Gradient for 'q/fully_connected/BiasAdd'"
2018-04-24 14:12:13,963 (tf_logging.py:160) Level 1: "  in  --> gradients/q/fully_connected/Relu_grad/ReluGrad:0"
2018-04-24 14:12:13,963 (tf_logging.py:160) Level 1: "  out --> gradients/q/fully_connected/BiasAdd_grad/tuple/control_dependency:0, gradients/q/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,966 (tf_logging.py:160) Level 1: "Gradient for 'q/fully_connected/MatMul'"
2018-04-24 14:12:13,966 (tf_logging.py:160) Level 1: "  in  --> gradients/q/fully_connected/BiasAdd_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,966 (tf_logging.py:160) Level 1: "  out --> gradients/q/fully_connected/MatMul_grad/tuple/control_dependency:0, gradients/q/fully_connected/MatMul_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,966 (tf_logging.py:160) Level 1: "Gradient for 'q/fully_connected/biases/read'"
2018-04-24 14:12:13,966 (tf_logging.py:160) Level 1: "  in  --> gradients/q/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,966 (tf_logging.py:160) Level 1: "  out --> gradients/q/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,968 (tf_logging.py:160) Level 1: "Gradient for 'q/Flatten/flatten/Reshape'"
2018-04-24 14:12:13,968 (tf_logging.py:160) Level 1: "  in  --> gradients/q/fully_connected/MatMul_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,968 (tf_logging.py:160) Level 1: "  out --> gradients/q/Flatten/flatten/Reshape_grad/Reshape:0"
2018-04-24 14:12:13,968 (tf_logging.py:160) Level 1: "Gradient for 'q/fully_connected/weights/read'"
2018-04-24 14:12:13,968 (tf_logging.py:160) Level 1: "  in  --> gradients/q/fully_connected/MatMul_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,968 (tf_logging.py:160) Level 1: "  out --> gradients/q/fully_connected/MatMul_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,969 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_2/Relu'"
2018-04-24 14:12:13,969 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Flatten/flatten/Reshape_grad/Reshape:0"
2018-04-24 14:12:13,969 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_2/Relu_grad/ReluGrad:0"
2018-04-24 14:12:13,971 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_2/BiasAdd'"
2018-04-24 14:12:13,971 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_2/Relu_grad/ReluGrad:0"
2018-04-24 14:12:13,971 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_2/BiasAdd_grad/tuple/control_dependency:0, gradients/q/Conv_2/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,975 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_2/Conv2D'"
2018-04-24 14:12:13,975 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_2/BiasAdd_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,975 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_2/Conv2D_grad/tuple/control_dependency:0, gradients/q/Conv_2/Conv2D_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,975 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_2/biases/read'"
2018-04-24 14:12:13,975 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_2/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,975 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_2/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,976 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_1/Relu'"
2018-04-24 14:12:13,976 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_2/Conv2D_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,976 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_1/Relu_grad/ReluGrad:0"
2018-04-24 14:12:13,976 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_2/weights/read'"
2018-04-24 14:12:13,976 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_2/Conv2D_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,976 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_2/Conv2D_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,978 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_1/BiasAdd'"
2018-04-24 14:12:13,978 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_1/Relu_grad/ReluGrad:0"
2018-04-24 14:12:13,978 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_1/BiasAdd_grad/tuple/control_dependency:0, gradients/q/Conv_1/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,982 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_1/Conv2D'"
2018-04-24 14:12:13,982 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_1/BiasAdd_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,982 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_1/Conv2D_grad/tuple/control_dependency:0, gradients/q/Conv_1/Conv2D_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,983 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_1/biases/read'"
2018-04-24 14:12:13,983 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_1/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,983 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_1/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,983 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv/Relu'"
2018-04-24 14:12:13,983 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_1/Conv2D_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,983 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv/Relu_grad/ReluGrad:0"
2018-04-24 14:12:13,984 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv_1/weights/read'"
2018-04-24 14:12:13,984 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv_1/Conv2D_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,984 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv_1/Conv2D_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,985 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv/BiasAdd'"
2018-04-24 14:12:13,985 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv/Relu_grad/ReluGrad:0"
2018-04-24 14:12:13,986 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv/BiasAdd_grad/tuple/control_dependency:0, gradients/q/Conv/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,989 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv/Conv2D'"
2018-04-24 14:12:13,989 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv/BiasAdd_grad/tuple/control_dependency:0"
2018-04-24 14:12:13,989 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv/Conv2D_grad/tuple/control_dependency:0, gradients/q/Conv/Conv2D_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,989 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv/biases/read'"
2018-04-24 14:12:13,989 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,989 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv/BiasAdd_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,990 (tf_logging.py:160) Level 1: "Gradient for 'q/Conv/weights/read'"
2018-04-24 14:12:13,990 (tf_logging.py:160) Level 1: "  in  --> gradients/q/Conv/Conv2D_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:13,990 (tf_logging.py:160) Level 1: "  out --> gradients/q/Conv/Conv2D_grad/tuple/control_dependency_1:0"
2018-04-24 14:12:14,105 (tf_logging.py:126) WARNING: "From /home/syedeqbal/.local/lib/python3.5/site-packages/tensorflow/python/ops/clip_ops.py:113: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead"
2018-04-24 14:12:14,155 (tf_logging.py:160) Level 1: "Created variable q/Conv/weights/Adam:0 with shape (1, 1, 2, 8) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1d55f8>"
2018-04-24 14:12:14,161 (tf_logging.py:160) Level 1: "Created variable q/Conv/weights/Adam_1:0 with shape (1, 1, 2, 8) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1d55f8>"
2018-04-24 14:12:14,165 (tf_logging.py:160) Level 1: "Created variable q/Conv/biases/Adam:0 with shape (8,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1d55f8>"
2018-04-24 14:12:14,173 (tf_logging.py:160) Level 1: "Created variable q/Conv/biases/Adam_1:0 with shape (8,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1d55f8>"
2018-04-24 14:12:14,182 (tf_logging.py:160) Level 1: "Created variable q/Conv_1/weights/Adam:0 with shape (3, 3, 8, 12) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1d55f8>"
2018-04-24 14:12:14,194 (tf_logging.py:160) Level 1: "Created variable q/Conv_1/weights/Adam_1:0 with shape (3, 3, 8, 12) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,201 (tf_logging.py:160) Level 1: "Created variable q/Conv_1/biases/Adam:0 with shape (12,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,216 (tf_logging.py:160) Level 1: "Created variable q/Conv_1/biases/Adam_1:0 with shape (12,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,220 (tf_logging.py:160) Level 1: "Created variable q/Conv_2/weights/Adam:0 with shape (3, 3, 12, 16) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,226 (tf_logging.py:160) Level 1: "Created variable q/Conv_2/weights/Adam_1:0 with shape (3, 3, 12, 16) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,238 (tf_logging.py:160) Level 1: "Created variable q/Conv_2/biases/Adam:0 with shape (16,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,242 (tf_logging.py:160) Level 1: "Created variable q/Conv_2/biases/Adam_1:0 with shape (16,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,250 (tf_logging.py:160) Level 1: "Created variable q/fully_connected/weights/Adam:0 with shape (576, 128) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,259 (tf_logging.py:160) Level 1: "Created variable q/fully_connected/weights/Adam_1:0 with shape (576, 128) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,267 (tf_logging.py:160) Level 1: "Created variable q/fully_connected/biases/Adam:0 with shape (128,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,276 (tf_logging.py:160) Level 1: "Created variable q/fully_connected/biases/Adam_1:0 with shape (128,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,280 (tf_logging.py:160) Level 1: "Created variable q/fully_connected_1/weights/Adam:0 with shape (128, 5) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,282 (tf_logging.py:160) Level 1: "Created variable q/fully_connected_1/weights/Adam_1:0 with shape (128, 5) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,285 (tf_logging.py:160) Level 1: "Created variable q/fully_connected_1/biases/Adam:0 with shape (5,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,289 (tf_logging.py:160) Level 1: "Created variable q/fully_connected_1/biases/Adam_1:0 with shape (5,) and init <tensorflow.python.ops.init_ops.Zeros object at 0x7f88cf1edbe0>"
2018-04-24 14:12:14,376 (dqn_agent.py:146) DEBUG: "loading checkpoint from checkpoints/data_file=7_17.npy,step_reward=-0.1,fast_fail=True,reuse_weights=False,test=False"
2018-04-24 14:12:14,378 (tf_logging.py:116) INFO: "Restoring parameters from checkpoints/data_file=7_17.npy,step_reward=-0.1,fast_fail=True,reuse_weights=False,test=False/best_reward-88"
2018-04-24 14:12:14,418 (dqn_agent.py:162) DEBUG: "var_name: q/Conv/weights:0, var_value [[[[-0.3842626   0.42710143 -0.48237562  0.7081746  -0.03765922
    -0.14166075 -0.6734322   0.8003955 ]
   [ 0.68954164 -0.01749362  0.72579926 -0.66363627  0.4244573
    -0.5211806  -0.7130368   0.69630194]]]]: "
2018-04-24 14:12:14,422 (dqn_agent.py:162) DEBUG: "var_name: q/Conv/biases:0, var_value [ 0.02014444 -0.16396086 -0.08457343 -0.07023297 -0.15090875  0.
  0.         -0.2088292 ]: "
2018-04-24 14:12:14,438 (dqn_agent.py:162) DEBUG: "var_name: q/Conv_1/weights:0, var_value [[[[ 1.57908976e-01  2.36263484e-01  1.65418655e-01  1.66984014e-02
     3.10316443e-01 -1.57538280e-01  1.48687407e-01  7.96430707e-02
     1.41747728e-01  7.87209272e-02 -5.90410717e-02  2.69192815e-01]
   [-4.75146830e-01  3.60437125e-01 -4.73256074e-02 -1.14829009e-02
    -2.61039317e-01 -1.11773521e-01 -1.54554939e+00  3.40772241e-01
    -1.08163707e-01 -1.97432563e-02 -1.08842731e-01 -4.61841673e-01]
   [ 2.70208478e-01  3.19436401e-01  1.10185698e-01 -1.55875877e-01
     8.88662338e-02 -2.32913256e-01  1.81336895e-01  5.41864848e-03
     7.87939783e-03 -4.39741127e-02 -2.80115157e-01  2.33532086e-01]
   [-8.24942961e-02 -6.80780932e-02  1.83544606e-01  2.38941908e-02
    -4.09093350e-01 -5.85916162e-01 -8.23739707e-01  1.90047368e-01
     1.90484256e-01 -3.60872708e-02  2.75956213e-01 -7.58629143e-01]
   [-5.99671751e-02  3.26964170e-01  3.32019001e-01  2.29968899e-03
     1.35696650e-01  1.10621769e-02 -1.23155564e-01  1.59698233e-01
    -1.33730128e-01  1.65672228e-02 -2.21143126e-01 -1.74409539e-01]
   [-1.16928458e-01 -1.69084474e-01  1.18552178e-01 -2.28864104e-02
    -1.73668787e-01  1.01377904e-01  1.59139305e-01  1.17340088e-02
    -1.37613475e-01 -3.11780572e-02 -1.55784696e-01 -2.68583000e-03]
   [-1.71064526e-01  1.66021168e-01  2.53989995e-02 -3.74134630e-02
     1.49930209e-01 -1.79436862e-01 -2.16339976e-02 -8.60414878e-02
     4.01067734e-03  1.64670467e-01 -9.94056985e-02 -1.33151576e-01]
   [-3.96222830e-01  4.18078601e-01 -4.08231951e-02  1.54351085e-01
    -2.58399755e-01 -3.74619424e-01 -6.96594536e-01  2.42288709e-01
    -4.08265879e-03 -1.51941583e-01 -2.87518322e-01 -4.04192805e-01]]

  [[-4.81365802e-04  2.49943599e-01 -5.47469705e-02 -2.30093434e-01
    -3.07046086e-01  2.65622288e-01  2.03240976e-01 -1.00699283e-01
    -1.69326454e-01  2.07257345e-01  4.39728908e-02  1.22928351e-01]
   [-2.36170888e-01 -3.85423899e-01 -1.71759933e-01  2.82904536e-01
     1.10593684e-01 -2.90672719e-01 -2.55927742e-01 -1.76460683e-01
     1.57845274e-01 -2.33886037e-02 -4.77025300e-01  5.46736992e-04]
   [ 2.27879971e-01  2.97392249e-01 -1.62640437e-01 -3.78578603e-01
    -1.50147574e-02  2.24969819e-01 -1.14409484e-01 -6.33420423e-02
    -1.73121333e-01  3.24507236e-01  9.10809264e-02  6.21810928e-02]
   [ 2.77494699e-01 -8.29813331e-02  2.26114467e-02  7.74464682e-02
     5.40722191e-01 -2.47540727e-01 -4.80735660e-01 -3.25099379e-01
     1.23093240e-01 -3.70132886e-02 -2.85582095e-01 -4.37713891e-01]
   [-2.29448423e-01 -2.63097435e-02 -2.72558272e-01 -1.08655781e-01
    -2.41464958e-01  1.02681980e-01 -5.77781275e-02 -1.27126481e-02
    -9.10791159e-02  2.18710765e-01 -5.19911125e-02 -1.48143291e-01]
   [-1.92211717e-02  1.19614035e-01  5.93356639e-02  6.83401227e-02
     7.57441223e-02 -1.78623825e-01 -1.40400827e-02  1.41271919e-01
    -9.76257548e-02  1.73211545e-01  9.61068869e-02 -1.10053718e-02]
   [-5.33091426e-02  1.31113112e-01 -4.51277643e-02 -1.15980051e-01
    -1.56379491e-02 -1.45686328e-01 -1.74439117e-01 -7.03044683e-02
     1.49843365e-01  5.55234253e-02  3.97729576e-02 -5.61143756e-02]
   [-3.86347562e-01 -5.64558804e-01 -1.67495802e-01  2.97553092e-01
     1.58062831e-01 -2.05908164e-01 -1.23315090e-02  4.68632095e-02
    -2.94786319e-02 -5.43889180e-02 -3.06776047e-01 -9.86050218e-02]]

  [[ 8.07159990e-02 -3.90751690e-01  1.59415185e-01  1.70167223e-01
    -9.30788219e-01 -3.45105171e-01 -6.81052282e-02  3.02871108e-01
     1.59005687e-01  1.01407938e-01  1.03068285e-01 -2.64251262e-01]
   [-6.32866696e-02  2.23434865e-01 -6.22349858e-01  2.63307065e-01
     1.09729804e-01  6.18282408e-02  2.49899969e-01 -5.60025312e-03
    -5.01511753e-01 -8.86207879e-01 -4.65429336e-01  2.88766831e-01]
   [-8.38905573e-02 -3.47698033e-01  2.66504884e-01  1.18181042e-01
    -7.11641967e-01 -1.75778285e-01 -2.32443169e-01  1.76154137e-01
    -1.28819570e-01  4.10540968e-01  8.14414248e-02 -2.42754057e-01]
   [-4.56881166e-01  1.61951751e-01 -2.26481333e-01 -2.14851081e-01
     4.64590549e-01  5.53538382e-01  3.44046026e-01 -4.90610510e-01
    -1.86957240e-01  8.51534382e-02  2.68984258e-01  1.64116785e-01]
   [-1.16558298e-01 -2.54598141e-01 -2.02699631e-01  1.37550578e-01
    -8.40490997e-01 -2.09543005e-01 -6.87994510e-02  3.63003433e-01
    -1.80544704e-02  5.63965924e-02 -2.04739556e-01  7.35405982e-02]
   [ 1.08801365e-01  3.61566246e-03 -1.01989627e-01 -9.79241878e-02
    -1.51516706e-01  1.32251561e-01  2.70187110e-02  1.21899873e-01
     1.59162372e-01 -8.13324675e-02  4.81122136e-02  1.86986625e-02]
   [-7.18268529e-02  3.86774540e-02  1.82534009e-01  1.39233470e-02
     1.60867900e-01  7.48507380e-02 -1.01700947e-01 -3.06328982e-02
     1.37778223e-01 -1.66307405e-01 -1.19131207e-01 -1.12202726e-01]
   [-1.21806897e-01  1.31165743e-01 -3.51859421e-01  2.93061972e-01
    -1.64214566e-01  1.56165928e-01  1.36923313e-01  3.13896000e-01
    -5.69873273e-01 -6.53537989e-01 -4.04817134e-01  4.67037439e-01]]]


 [[[-1.10831246e-01  4.06544328e-01 -2.78275669e-01  5.89123294e-02
    -2.64746219e-01 -1.72582537e-01 -3.94065231e-01 -1.22421697e-01
    -5.22435188e-01 -3.13763678e-01  6.14634454e-01  3.57794017e-01]
   [ 2.46257231e-01 -2.86270261e-01  1.34583876e-01 -9.31663960e-02
     3.13513339e-01  2.58915693e-01 -1.20839663e-02 -1.53334229e-03
     1.19409924e-02 -3.03014547e-01  1.47201136e-01  2.82972544e-01]
   [-1.73776329e-01  4.44217622e-01 -8.40070769e-02 -9.42963362e-02
    -1.05975233e-01  1.71118721e-01 -2.07444191e-01 -2.10384458e-01
    -1.13536513e+00 -6.00215085e-02  6.36085331e-01  8.77584741e-02]
   [ 3.55788946e-01 -3.91922683e-01  4.43024421e-03  2.20266983e-01
     6.15502417e-01  5.79290211e-01  1.28737241e-01  3.02889049e-01
     3.20146501e-01  3.20414573e-01 -1.35631561e-01 -3.51263434e-01]
   [ 4.62412387e-02  3.66610229e-01 -3.91889829e-03 -2.44033448e-02
    -2.05416903e-01 -8.60904455e-02 -3.02614897e-01 -8.72692466e-02
    -1.12489796e+00 -2.12111339e-01  3.80114228e-01  3.13941479e-01]
   [-4.14004177e-02 -7.58840665e-02 -1.63450032e-01  8.28725696e-02
     5.11182845e-02  9.55280364e-02 -1.82228386e-02  1.80264413e-01
    -2.79470533e-02  1.19096726e-01  4.23515290e-02 -7.85641968e-02]
   [ 7.92072415e-02 -2.74528563e-03  6.48586750e-02 -3.33484560e-02
     6.62521720e-02  1.33071750e-01 -1.74410746e-01  5.30443192e-02
    -9.45934281e-02 -1.14654280e-01 -1.38677761e-01  2.05377191e-02]
   [ 3.39709550e-01  1.45732924e-01 -1.99662581e-01 -9.62167978e-02
    -1.61879808e-02  7.49328732e-02 -1.13791876e-01  1.55823052e-01
    -4.69134718e-01 -4.78766292e-01  4.52500194e-01  4.36450273e-01]]

  [[-1.19477019e-01 -1.69855766e-02  2.40208060e-02 -1.07246213e-01
    -4.46797311e-02 -3.60024184e-01  1.27797604e-01 -1.04783550e-01
    -1.47091985e-01 -2.19602793e-01  4.05334979e-01 -1.43424109e-01]
   [ 1.25179350e-01 -3.05135816e-01 -1.00171492e-01 -2.85954535e-01
     5.49582206e-02  1.63014624e-02  2.39172727e-01  3.92014980e-02
    -3.88496876e-01  5.69873810e-01 -1.89716354e-01  2.52970289e-02]
   [-2.62796611e-01 -1.41272351e-01 -3.35058719e-01 -3.53101701e-01
     1.51863582e-02 -5.03266871e-01  2.70671815e-01  7.99473897e-02
    -1.01253837e-01  1.48031916e-02  5.01799524e-01 -1.02065891e-01]
   [ 4.14337218e-01 -1.40646413e-01 -2.05751732e-01  1.24890648e-01
     8.32057819e-02  1.66901171e-01  5.52104712e-01  6.81142211e-02
    -3.18518966e-01  5.70630968e-01 -2.47131642e-02  3.33200574e-01]
   [ 6.70211464e-02 -3.95243280e-02 -1.68939441e-01 -3.23845088e-01
     2.37793878e-01 -4.73449469e-01  6.61735758e-02  9.40135643e-02
    -2.42645964e-01 -2.68569924e-02  2.59855300e-01 -4.20236826e-01]
   [ 1.71099395e-01  1.09755486e-01  1.38165027e-01 -5.69903851e-03
     1.75003201e-01  1.44286782e-01 -5.61501086e-02 -2.53363997e-02
     5.38746715e-02 -7.73933530e-02 -1.31253272e-01  1.33172303e-01]
   [-4.72038090e-02 -3.75347286e-02  3.48986536e-02  2.25902796e-02
    -1.46622568e-01  9.67561603e-02  8.91182125e-02  1.71213776e-01
     4.89567220e-02 -2.77263671e-02 -8.67928043e-02 -1.29502416e-02]
   [ 2.18246818e-01 -1.84651613e-01 -6.65014684e-02 -2.60886580e-01
    -4.97232340e-02 -1.35398373e-01  4.38994855e-01 -3.72385457e-02
    -3.45756888e-01  5.14636099e-01  1.35959476e-01 -2.09816843e-01]]

  [[-1.58079311e-01 -7.56787583e-02 -1.68546528e-01 -2.70298421e-02
     1.31556034e-01 -8.58368278e-02 -2.53347725e-01  2.20081866e-01
    -3.86355132e-01  2.87495166e-01 -3.26335371e-01  1.23117574e-01]
   [ 2.69421726e-01  3.21531504e-01 -1.16314530e-01  2.24157304e-01
     8.51133764e-02  2.64326781e-02 -4.24365140e-02 -1.46445587e-01
     5.18641286e-02  1.16175950e-01 -8.25552642e-02  3.62323821e-02]
   [-2.65971154e-01  2.46953741e-02  2.05064900e-02 -1.21199377e-01
     1.63869381e-01  1.76464841e-01 -3.59445214e-01  1.98104769e-01
    -5.82658470e-01  1.49706587e-01 -2.36881495e-01 -2.44553760e-02]
   [ 9.56592739e-01  2.65036970e-01  1.12604156e-01 -2.22745031e-01
     3.43698114e-01  4.05874342e-01  1.75246626e-01  4.03815582e-02
     4.11952168e-01 -2.24570632e-01  2.31736019e-01  1.32455304e-01]
   [-2.75441080e-01 -1.00864500e-01 -8.33010003e-02 -3.06578912e-02
     1.84164435e-01 -1.41497791e-01 -2.98967183e-01 -6.56041428e-02
    -1.66233048e-01  9.82314423e-02 -3.20792049e-01 -8.75578076e-02]
   [-1.57247439e-01 -1.43974036e-01 -1.11414373e-01  1.74075186e-01
     9.05815363e-02  6.04171902e-02 -7.25063831e-02  1.98715031e-02
     1.43994957e-01 -3.13255191e-02  8.25239122e-02 -1.81371436e-01]
   [ 7.57984817e-02  9.56378281e-02 -3.11301649e-02 -1.04971677e-02
    -3.86978686e-02 -1.40289605e-01  5.17287850e-02  1.66545480e-01
    -2.83560157e-02 -2.57818401e-02  1.71208739e-01 -6.82893768e-02]
   [ 1.32579818e-01  2.69180059e-01 -1.62481934e-01  2.26246431e-01
     3.73023868e-01 -8.84454772e-02 -1.84525579e-01  7.32530728e-02
    -1.31974727e-01  7.15410858e-02 -6.50875792e-02  1.61100522e-01]]]


 [[[-7.15327263e-02  1.91651165e-01  6.46197140e-01  7.00611174e-02
    -7.84784332e-02  5.75795397e-02 -2.38748983e-01  1.22987449e-01
    -9.35315788e-02  3.17766398e-01 -2.57059604e-01 -5.74876666e-02]
   [ 2.58538425e-01  4.55794185e-02 -7.70657808e-02  2.32896894e-01
     2.26833567e-01  3.12318206e-01  1.90086424e-01  3.33856583e-01
    -3.01765829e-01 -2.50239926e-03  4.49960567e-02  8.56213793e-02]
   [-5.74929006e-02  1.47675052e-01  6.59429729e-01  2.49413356e-01
    -1.64370120e-01  3.07702750e-01 -9.26343426e-02 -1.20850414e-01
    -2.99005699e-03  1.65415227e-01 -2.62678385e-01 -3.99998814e-01]
   [ 1.12412699e-01 -8.81603733e-02 -6.83575153e-01 -4.22962829e-02
     1.59264386e-01  1.02377713e+00  6.13784432e-01  6.02126658e-01
     2.92520344e-01 -1.96503118e-01  7.06683670e-04  2.72037834e-01]
   [-2.72509251e-02  7.19174296e-02  4.59328920e-01  1.80149525e-01
    -6.07462525e-02  1.88597783e-01  1.75807122e-02 -2.05919836e-02
     2.50963931e-04  1.41671419e-01 -1.74153298e-01  1.11124869e-02]
   [-1.27826005e-01 -1.49427399e-01 -3.03150415e-02  7.10646808e-03
     1.14063621e-02 -5.39898574e-02  1.54998302e-02 -4.40126359e-02
     1.58390731e-01  2.54238993e-02  4.43068147e-02  1.03162736e-01]
   [-4.40144241e-02 -8.69340077e-02 -4.61161435e-02 -1.58744410e-01
    -2.08557397e-02  1.25849217e-01 -7.77893811e-02  7.65181184e-02
     1.06382817e-02  1.45737827e-01 -1.70219988e-02 -2.56664008e-02]
   [ 7.30122030e-02  5.15643507e-02  3.86561513e-01  3.42454046e-01
     1.78751603e-01  3.17977548e-01  2.95535058e-01  2.71570887e-02
    -3.26570332e-01  1.62645295e-01 -1.92404538e-01  1.07023790e-01]]

  [[ 1.90208510e-01 -5.48228547e-02  1.98443457e-01  4.55673710e-02
    -2.05473259e-01 -1.24985248e-01  1.90139115e-01  2.02983931e-01
     4.47370708e-01 -6.01485223e-02 -1.77838162e-01  1.45341352e-01]
   [-8.84884149e-02  4.43429090e-02  1.05852164e-01 -4.73271534e-02
     3.92974652e-02 -1.96201548e-01  2.40134597e-01 -7.55818337e-02
     4.25930351e-01  1.13245048e-01 -1.73143923e-01  1.38743401e-01]
   [ 1.89319581e-01  2.96436972e-03  7.18576163e-02 -1.56092137e-01
    -1.36328503e-01 -4.07687187e-01  6.22976869e-02  4.35390435e-02
     5.17005801e-01 -1.38652012e-01 -2.29008690e-01 -7.95988068e-02]
   [ 4.29514527e-01 -1.48259133e-01  1.45888822e-02 -2.74065942e-01
    -1.24932110e-01 -4.14965063e-01 -7.07687855e-01  2.17434123e-01
     5.41878752e-02 -1.05230138e-01  1.67301103e-01 -3.81813377e-01]
   [-9.54849944e-02  1.12577006e-01  7.53609240e-02  1.40903324e-01
    -5.84919974e-02 -1.72238991e-01  3.49680513e-01 -1.24528944e-01
     3.24048936e-01  1.32916808e-01 -6.29770905e-02  2.24303812e-01]
   [-1.07972175e-02 -7.06320629e-02  4.71407175e-03  1.69853061e-01
    -1.48884892e-01  1.18483007e-01  1.67882472e-01 -1.26603708e-01
    -1.32022351e-02  1.40404075e-01  1.60586476e-01  6.81694150e-02]
   [-1.07416615e-01 -2.72439420e-02 -8.22232962e-02  8.92572403e-02
     9.25876498e-02  6.23116493e-02  4.98044044e-02 -1.37133390e-01
    -1.32860631e-01  1.78674817e-01 -5.09697199e-02 -8.95811617e-03]
   [-2.24528108e-02 -1.26325697e-01  1.70466620e-02 -2.13208601e-01
     2.80158278e-02 -1.77380994e-01  2.39674989e-02  4.48514447e-02
     5.11303902e-01  2.07002331e-02  3.51129076e-03  1.81627106e-02]]

  [[-9.28699225e-02 -3.64002138e-02  4.03782934e-01  2.57309586e-01
     8.02107304e-02 -2.22658068e-01  1.55284435e-01 -1.44406855e-01
     2.19841331e-01  1.12524018e-01  6.33226633e-02 -6.17507659e-02]
   [-1.25230074e-01  2.55384475e-01 -1.07362561e-01 -2.12351844e-01
     3.91005963e-01 -1.97019409e-02 -6.18963540e-02 -8.47284943e-02
     2.93940932e-01  1.92469601e-02 -3.42755288e-01  3.07034925e-02]
   [-1.22168943e-01 -1.58553600e-01  1.69391632e-01  7.97242969e-02
     4.54854965e-02  3.90238222e-03  3.37346047e-02 -5.77174388e-02
     4.81753536e-02 -1.15558408e-01 -5.14879599e-02  1.32555038e-01]
   [ 1.05236016e-01  6.76941812e-01 -4.01293874e-01 -5.76217473e-01
     4.80776370e-01 -3.00975591e-01 -6.55085504e-01  7.66352937e-02
     6.54669046e-01 -2.05003470e-01 -4.11959380e-01  1.20584302e-01]
   [-4.05460745e-02  9.47164968e-02  2.93236285e-01  2.38583580e-01
     2.87667036e-01 -4.74400446e-02  2.80839145e-01 -1.77495599e-01
     2.26608217e-02  1.87707558e-01  1.13622673e-01 -5.95980436e-02]
   [-5.41606992e-02 -9.23794135e-02  6.00416213e-02 -1.34202182e-01
    -1.12752758e-01 -9.00207460e-02  1.42227948e-01  6.48628920e-02
     6.48780465e-02  3.62690389e-02  1.16524458e-01 -1.05321914e-01]
   [-6.08861223e-02 -2.42796093e-02 -7.98016414e-02 -2.09693015e-02
     5.30975908e-02 -9.63776410e-02  2.78834254e-02 -1.61698297e-01
     7.26583898e-02 -3.16943973e-02 -6.96819574e-02 -6.99496195e-02]
   [-3.04783821e-01  1.73953310e-01  2.09537998e-01  6.75738901e-02
     3.82591993e-01 -4.07799840e-01  1.82924703e-01 -1.66502431e-01
     2.60014921e-01  5.93364723e-02 -1.69355959e-01  1.73690841e-01]]]]: "
2018-04-24 14:12:14,445 (dqn_agent.py:162) DEBUG: "var_name: q/Conv_1/biases:0, var_value [ 0.10535506 -0.14567755  0.01438164 -0.03695296 -0.03497759  0.03599799
 -0.046046   -0.02084113 -0.00026308  0.00947261 -0.01138524 -0.01994674]: "
2018-04-24 14:12:14,457 (dqn_agent.py:162) DEBUG: "var_name: q/Conv_2/weights:0, var_value [[[[-2.41449669e-01 -5.34254722e-02  2.55125999e-01 ... -2.69252002e-01
     1.46744326e-01 -1.64181128e-01]
   [ 3.18062067e-01 -1.15118206e-01 -2.17820838e-01 ... -3.29854101e-01
    -3.92338514e-01 -2.65229523e-01]
   [-5.22299826e-01 -1.89754181e-02  1.51664674e-01 ...  9.65580866e-02
    -1.75353527e-01 -9.42160115e-02]
   ...
   [-1.99625611e-01  4.32765894e-02 -2.35206008e-01 ... -9.32345837e-02
     9.39134210e-02 -5.16004823e-02]
   [ 3.13908219e-01  2.93489009e-01 -2.10851327e-01 ... -6.10644341e-01
     3.49426359e-01 -4.78882790e-01]
   [ 2.39711061e-01  1.50681445e-02  1.61193684e-01 ... -2.73865700e-01
     4.32406425e-01 -4.51224983e-01]]

  [[-1.33026466e-01 -1.26625255e-01 -1.98688254e-01 ... -9.74047601e-01
    -1.87634528e-01  1.84524074e-01]
   [ 2.83999264e-01  2.26553828e-02 -3.54044765e-01 ... -6.46213116e-03
    -2.11558178e-01  2.04273850e-01]
   [-3.70430276e-02  1.96939632e-01  3.95578384e-01 ...  5.66437878e-02
    -1.21716082e-01  1.06804423e-01]
   ...
   [ 2.66831547e-01  2.59639800e-01  3.08012962e-01 ... -1.44180447e-01
    -1.77539065e-01 -2.06214428e-01]
   [-1.85881168e-01 -4.68818955e-02 -1.93581104e-01 ... -2.04702124e-01
    -2.13256478e-01 -3.69633846e-02]
   [-7.06315711e-02  1.52284265e-01 -1.02523685e-01 ... -2.39206836e-01
    -1.99740261e-01  8.01080838e-02]]

  [[ 5.15035391e-01 -1.32619287e-03 -4.36251193e-01 ...  3.55067588e-02
    -2.86883861e-01  3.55606645e-01]
   [ 1.64627984e-01  3.28141272e-01  2.96485028e-03 ...  1.18057117e-01
     1.70026034e-01  3.83506157e-02]
   [ 2.73051471e-01  9.11929905e-02 -1.23570286e-01 ... -4.55043502e-02
    -2.90703207e-01  2.29321286e-01]
   ...
   [-3.71570349e-01 -4.55673128e-01  9.25039649e-02 ...  3.10655266e-01
    -4.70931619e-01  9.37193856e-02]
   [ 1.93520069e-01  1.41828761e-01 -2.51524955e-01 ...  1.39179960e-01
    -7.54920417e-04  2.29033753e-01]
   [-6.83036670e-02 -5.53677864e-02  2.00436831e-01 ... -3.11326742e-01
    -7.01230243e-02 -2.27393553e-01]]]


 [[[-1.13352910e-01 -4.93823849e-02 -1.17840301e-02 ...  1.84063613e-01
     5.83508946e-02  5.41840903e-02]
   [ 5.32101631e-01  1.57478377e-01  1.97748706e-01 ... -3.54088485e-01
     1.64472193e-01 -3.66036624e-01]
   [ 3.19313318e-01  3.11722700e-03 -6.30381852e-02 ...  4.19514596e-01
     1.83600709e-01  9.88867134e-02]
   ...
   [ 1.41395271e-01  1.70138389e-01 -2.26125807e-01 ...  1.45689189e-01
    -1.10882245e-01 -1.23609863e-01]
   [-4.36667830e-01 -5.07290125e-01 -8.32695886e-02 ... -1.28299683e-01
    -4.14282441e-01  1.80860355e-01]
   [ 5.84287290e-03  6.24661222e-02  1.24327779e-01 ...  1.07817523e-01
     3.48861925e-02  1.30702853e-01]]

  [[ 5.29435463e-02 -2.28952482e-01 -2.50350654e-01 ...  5.89164197e-02
    -2.03094691e-01 -1.79305881e-01]
   [ 1.51476309e-01 -1.45623565e-01  1.17405415e-01 ...  1.41527578e-01
    -1.62230343e-01  1.04582615e-01]
   [-2.75358055e-02 -2.52247639e-02  1.51612878e-01 ... -1.33030340e-01
     6.44853562e-02 -3.45877856e-01]
   ...
   [-5.23115575e-01 -2.05495376e-02  1.49583414e-01 ... -7.25965202e-02
     1.04104556e-01 -1.42992765e-01]
   [-4.89017069e-02 -1.67753696e-01 -1.16551757e-01 ... -2.98356324e-01
    -5.53530753e-01 -1.19592041e-01]
   [-1.04106680e-01 -4.20228571e-01 -9.01095197e-02 ...  4.57712501e-01
    -1.09294690e-01 -5.93658499e-02]]

  [[ 1.01441694e-02  8.15739483e-02  4.32100385e-01 ...  1.78149238e-01
     1.10713609e-01  2.55182028e-01]
   [ 2.21322179e-01  3.03741712e-02  2.27362514e-01 ...  6.87184632e-02
     1.87759563e-01  2.78439760e-01]
   [ 1.69676274e-01 -8.48821700e-02  4.72275823e-01 ... -1.00616179e-01
    -3.12018320e-02 -1.48578644e-01]
   ...
   [ 2.51435697e-01 -1.36775091e-01  3.42942446e-01 ... -6.30166888e-01
     1.67863965e-01  3.05713028e-01]
   [-3.74233127e-01  4.31891799e-01  4.12264407e-01 ... -5.34088671e-01
    -8.62760171e-02  2.88886309e-01]
   [-2.76844949e-01 -1.19374216e-01 -3.03454727e-01 ...  1.64526358e-01
    -5.92421293e-02 -2.24565137e-02]]]


 [[[-1.04004666e-01 -2.06602126e-01 -3.31472129e-01 ...  4.92547989e-01
    -8.65056440e-02  4.35116254e-02]
   [-2.30724350e-01  3.26067954e-01  3.11782211e-01 ...  9.29225683e-02
    -2.47843415e-01  2.89765865e-01]
   [ 1.21429926e-02 -2.04406664e-01 -4.54302758e-01 ...  9.83411446e-02
     1.85889214e-01 -4.05271739e-01]
   ...
   [ 2.71381974e-01  7.06129149e-02 -3.89743179e-01 ...  4.25456434e-01
     1.79809779e-01 -2.13114783e-01]
   [ 3.85508477e-03  1.28412619e-01 -4.03910339e-01 ...  5.76406009e-02
    -3.60745579e-01  7.25495890e-02]
   [-2.02729613e-01 -9.56437588e-02  4.22800303e-01 ... -8.59438479e-02
    -8.12424272e-02  1.41532451e-01]]

  [[-1.13986187e-01  2.11814567e-01 -6.38982505e-02 ...  5.85329592e-01
    -1.03516385e-01 -8.57791975e-02]
   [-2.36424044e-01 -3.72458138e-02 -1.55087903e-01 ... -2.91638434e-01
    -2.27965474e-01  4.92279232e-02]
   [ 8.06549340e-02 -4.24761266e-01 -1.15729570e-01 ... -5.31054854e-01
     6.68823302e-01  2.78068274e-01]
   ...
   [-9.66653004e-02 -2.67881870e-01  1.85834616e-02 ... -1.61479469e-02
     3.83571893e-01 -1.51251070e-02]
   [-1.02213061e+00 -8.49537179e-03  1.81891561e-01 ... -4.72231269e-01
    -1.21338621e-01  9.14643556e-02]
   [ 1.30873531e-01 -1.27570033e-01  3.27024749e-03 ... -1.03149697e-01
     1.04601018e-01 -5.49281612e-02]]

  [[ 2.54847407e-01  2.26196617e-01 -1.58632189e-01 ...  2.00050205e-01
    -5.06856799e-01 -1.81164950e-01]
   [-1.92876756e-01 -1.04030281e-01  4.02533561e-01 ... -2.60307193e-01
     1.81405261e-01  2.12030597e-02]
   [-1.61670640e-01 -6.35528147e-01  8.20311680e-02 ... -1.84406593e-01
    -5.03129184e-01 -1.32312894e-01]
   ...
   [-4.56179947e-01 -1.06577270e-01  3.37775759e-02 ...  2.52632797e-01
    -4.07951139e-03  5.92708737e-02]
   [-6.83571100e-01  2.91390657e-01  4.16576743e-01 ... -3.96494091e-01
    -6.72485173e-01 -1.42043337e-01]
   [ 1.25826284e-01  5.65347522e-02  2.24754781e-01 ...  1.30325392e-01
     1.94313433e-02  2.74663746e-01]]]]: "
2018-04-24 14:12:14,464 (dqn_agent.py:162) DEBUG: "var_name: q/Conv_2/biases:0, var_value [ 0.11895626  0.24126464  0.04300895  0.22310545 -0.2430591  -0.22689237
  0.45065308  0.08348394  0.3702053   0.13443358  0.43161657  0.33025563
  0.20172037  0.1819763   0.1345724  -0.07143605]: "
2018-04-24 14:12:14,471 (dqn_agent.py:162) DEBUG: "var_name: q/fully_connected/weights:0, var_value [[ 0.23050644  0.01994356 -0.36850002 ...  0.06404281 -0.28993225
  -0.24448916]
 [ 0.06606911  0.0396628   0.02745071 ... -0.02488617  0.1913353
  -0.5438785 ]
 [-0.04089682  0.05491148  0.07513998 ... -0.09645336  0.17715548
  -0.53310746]
 ...
 [-0.8391496  -0.07790687 -0.2952602  ... -0.0121758  -0.31202227
   0.03637012]
 [-0.20549847  0.08081612  0.15979487 ... -0.0629594   0.27244717
  -0.0375578 ]
 [ 0.13435096  0.00546949  0.02023502 ... -0.1010722  -1.3518869
   0.5769472 ]]: "
2018-04-24 14:12:14,478 (dqn_agent.py:162) DEBUG: "var_name: q/fully_connected/biases:0, var_value [ 0.36725098 -0.00957042 -0.53188896 -0.02027706 -0.2297823   0.5917957
  0.27515936 -0.01725937  0.14079165 -0.27893326 -0.01430109 -0.01365287
  0.40094674  0.458554   -0.30088088 -0.3032073   0.30065247  0.113043
 -0.06180681  0.08631813  0.03617646  0.5287037  -0.55937034 -0.3435705
 -0.5580117  -0.01647493  0.15452044  0.24080233  0.02292323 -0.22531638
  0.28286642 -0.01600578 -0.56755364  0.32128954 -0.20352238 -0.01077665
 -0.0988888   0.2779302   0.22951114 -0.11411165 -0.33250073  0.44001615
  0.32495728  0.08392926 -0.00632227  0.15183495  0.31513324 -0.48193955
 -0.3429818   0.27584788 -0.02309261  0.14946471  0.38567027 -0.2057611
 -0.15081486 -0.5855758  -0.02096778 -0.5055112  -0.11600184 -0.34131455
  0.23414946  0.07400464  0.19944489 -0.0646525  -0.29653051 -0.00985826
 -0.02057208  0.00603723 -0.00992004 -0.28512323  0.1033206  -0.13445479
  0.14007077  0.54190993  0.16918707  0.05420721 -0.05212287  0.2807009
  0.4969421  -0.23901068  0.28754887 -0.09464515 -0.5021092  -0.3413578
 -0.2848077  -0.04237092 -0.582475   -0.3953621  -0.01612202 -0.01821942
 -0.01286166 -0.51564294 -0.04988134  0.5129274  -0.0167745   0.38166595
 -0.12671225 -0.01711439  0.15363972  0.2643233  -0.18248458  0.06745163
 -0.01670748 -0.00742324  0.05437957 -0.41286448 -0.40868005  0.5687168
  0.23134357  0.39478546 -0.08628965 -0.279669   -0.24108958 -0.01603
 -0.12005944  0.10043397  0.1603841  -0.02543154 -0.05282281 -0.43251607
 -0.13380371 -0.0285012   0.04026359 -0.09882317 -0.00938291 -0.02169454
 -0.04231279 -0.4575058 ]: "
2018-04-24 14:12:14,488 (dqn_agent.py:162) DEBUG: "var_name: q/fully_connected_1/weights:0, var_value [[ 1.65600684e-02  8.21991544e-03  1.19407875e-02 -1.52987232e-02
  -1.48181391e+00]
 [ 1.90950297e-02  1.18576542e-01  5.77700324e-02 -1.52934566e-01
   1.37765929e-01]
 [-1.68544799e-02 -1.64275058e-02  3.20471413e-02  2.15784740e-02
   3.94471228e-01]
 [-1.67746797e-01 -1.40990898e-01  8.08826461e-02 -5.10361157e-02
   1.31694332e-01]
 [-3.57777983e-01 -3.44890684e-01  1.45926863e-01 -2.96965718e-01
  -1.70494378e-01]
 [ 3.14272612e-01  2.80956894e-01  3.24050725e-01  3.48681182e-01
   8.38135928e-02]
 [ 3.63558740e-03  4.17484064e-03  2.06496101e-02  1.50509449e-02
  -3.95924002e-01]
 [-4.26310971e-02 -1.02004692e-01  7.76893720e-02  1.05672993e-01
   1.27461761e-01]
 [ 1.46346524e-01  1.13069788e-02  1.73250854e-01  4.07079933e-03
   6.00470193e-02]
 [ 4.72487230e-03  1.71657130e-02 -2.25282796e-02  7.16674142e-03
   1.29911864e+00]
 [ 1.70211568e-02  1.03218667e-02  1.02940038e-01 -1.83181137e-01
   3.73155414e-03]
 [ 1.14322685e-01  5.62420972e-02  1.12488516e-01 -2.76166218e-04
   1.53054953e-01]
 [ 2.46371955e-01  2.42954955e-01  2.49446303e-01  2.51843601e-01
   5.14376201e-02]
 [ 2.74960965e-01  3.84722114e-01  2.84657419e-01  2.33405679e-01
   5.81559874e-02]
 [ 6.26749359e-03  4.03657556e-02  1.23744868e-02  1.67631656e-02
   1.27137327e+00]
 [-1.26388475e-01 -2.44682074e-01 -5.14499024e-02 -2.72055149e-01
  -2.01021042e-03]
 [ 3.16045843e-02  8.85612238e-03 -2.87955860e-03 -1.16332136e-02
  -2.51959026e-01]
 [ 1.44887343e-02 -1.23441787e-02 -8.41094647e-03 -1.44999195e-02
  -4.26376224e-01]
 [-6.30260333e-02 -1.11343838e-01  7.03793541e-02  7.07648844e-02
  -1.50890741e-02]
 [ 1.34927509e-02 -1.11334100e-02  1.59910321e-02  2.39011087e-02
  -8.58882606e-01]
 [ 9.20458212e-02  1.18108295e-01  4.68541235e-02  8.27357918e-02
   3.63065511e-01]
 [ 8.31320230e-03  2.23744288e-02  7.38444505e-03  5.30262664e-03
  -4.96045530e-01]
 [-2.89638024e-02 -7.45551195e-03 -1.92802120e-02  1.34054665e-03
   2.50998646e-01]
 [-1.18887685e-02  8.84941686e-03  1.46514801e-02 -7.62394071e-03
   6.30913734e-01]
 [-4.82296245e-03  2.55959178e-03 -2.57881787e-02 -3.60355005e-02
   6.27395391e-01]
 [ 5.19447066e-02  1.73286512e-01  1.32516026e-01  3.42205763e-02
  -7.46701956e-02]
 [-7.37826992e-03  2.28777733e-02  8.99024960e-03 -1.52714392e-02
  -7.71260321e-01]
 [ 1.45536199e-01  2.41292372e-01  1.89142779e-01  2.17621639e-01
   8.21681097e-02]
 [ 1.61166996e-01 -3.15095596e-02 -9.59682092e-02 -3.66463773e-02
   2.36271191e-02]
 [-6.86036237e-03 -4.14890721e-02 -1.37462653e-02  2.69596744e-02
   7.08291292e-01]
 [ 1.85471550e-01  1.57760501e-01  1.60610586e-01  1.64407998e-01
   6.09930232e-02]
 [-1.78311735e-01  2.60802414e-02  1.01164736e-01 -1.24185853e-01
   5.65709807e-02]
 [ 2.80439761e-02 -5.15293293e-02 -5.45181558e-02  3.20061706e-02
   2.40598425e-01]
 [-4.91542742e-02 -2.78870799e-02  3.94679606e-02 -3.86412884e-03
  -3.09400111e-01]
 [-2.14788709e-02 -2.08785851e-03 -2.83991988e-03 -1.08700609e-02
   3.07833374e-01]
 [-1.47071078e-01  8.23467523e-02  1.52535066e-01 -9.68541950e-02
  -1.45330802e-01]
 [-4.79916530e-03  1.78459510e-02  1.50136894e-03 -1.56141864e-02
   1.59608793e+00]
 [ 1.10883657e-02 -1.45671545e-02 -2.03204509e-02  1.59661248e-02
  -4.28157926e-01]
 [ 2.09581971e-01  2.66255260e-01  2.54919678e-01  2.40095958e-01
   8.90041068e-02]
 [-5.73571622e-02 -3.00303120e-02 -1.68957021e-02  4.20386232e-02
  -4.38913405e-01]
 [-5.36596216e-03  1.25923278e-02 -2.02285070e-02 -6.35271845e-03
   5.56074560e-01]
 [-4.28210106e-03  1.87039119e-03  1.48715135e-02  1.52901243e-02
  -4.21740592e-01]
 [ 1.82804335e-02  8.08990211e-04  3.54676740e-03 -1.07891122e-02
  -5.95536470e-01]
 [ 7.04839602e-02  1.89517647e-01  1.38030320e-01  1.34698942e-01
   7.18456751e-04]
 [-9.47529301e-02  1.83694080e-01  9.80010852e-02  1.62337556e-01
   1.12958059e-01]
 [ 2.50934158e-02 -2.03973055e-02 -2.96321362e-02 -2.23056339e-02
  -4.28137153e-01]
 [ 3.32727611e-01  3.40673864e-01  3.23047638e-01  3.32426995e-01
   1.93948999e-01]
 [-7.35085785e-01 -7.84761369e-01 -8.37429523e-01 -9.24716890e-01
  -2.16755629e-01]
 [ 2.61724815e-02 -2.82746321e-03 -1.48569187e-02 -1.51812080e-02
   7.44495213e-01]
 [-2.32015867e-02 -2.71017943e-02 -9.38411336e-03 -7.39775086e-03
  -4.37539905e-01]
 [ 5.94163984e-02  1.75790623e-01  2.00553626e-01 -1.13552608e-01
   1.48058638e-01]
 [-1.98653415e-01  9.17177871e-02  5.78298345e-02 -1.05380856e-01
  -4.08814102e-02]
 [ 5.91143779e-03  9.20170173e-03 -3.34189832e-02 -4.93440591e-02
  -4.39488649e-01]
 [-5.68043664e-02  5.59088700e-02 -5.68129355e-03  2.43952088e-02
   5.43161333e-01]
 [ 3.52141112e-02 -2.45916452e-02  2.45184880e-02  4.78404947e-03
   1.70202851e+00]
 [ 3.67120374e-03  3.90253868e-03  4.82603395e-03 -1.86242431e-03
   5.03396034e-01]
 [-4.92724292e-02 -2.41411403e-01 -7.42048249e-02  1.45813704e-01
   6.95984602e-01]
 [-2.26312838e-02 -1.12876380e-02 -1.75849088e-02 -1.73165444e-02
   3.54115278e-01]
 [ 1.43038571e-01 -1.89111363e-02  1.17659539e-01 -9.38276276e-02
   4.79829833e-02]
 [ 2.45378949e-02 -1.61405858e-02 -9.37791821e-03 -2.06089462e-03
   4.45666552e-01]
 [ 4.90463059e-03  3.09183765e-02  3.97181734e-02 -1.87212434e-02
  -5.81464708e-01]
 [-2.28448194e-02 -2.32254174e-02 -5.49036497e-03  6.56349957e-03
  -3.33657473e-01]
 [-2.65996102e-02 -1.87421236e-02 -4.56076907e-03  2.43732259e-02
  -1.31189203e+00]
 [-1.61757335e-01 -1.10860668e-01 -1.22501865e-01 -2.25760221e-01
  -6.95561394e-02]
 [ 3.51461349e-03 -4.97732460e-02  1.91589836e-02  6.07078820e-02
   1.91401219e+00]
 [-2.25853935e-01  7.88305048e-03 -2.28978135e-02 -2.30104268e-01
  -1.58818513e-01]
 [ 1.48835659e-01 -1.65559083e-01 -1.71363503e-01  2.04437017e-01
   1.99291036e-01]
 [-3.53766978e-02  4.05849069e-02  2.22982913e-02  1.93209834e-02
  -1.31091595e+00]
 [-6.16481416e-02  8.60585272e-02  8.72934237e-02  1.96763799e-01
   2.05595329e-01]
 [-8.19007158e-02 -1.60992965e-01 -1.73830286e-01  2.76336391e-02
  -1.71854272e-02]
 [-1.99638004e-03 -1.91598607e-03  1.91389006e-02  1.99125130e-02
  -9.05138612e-01]
 [ 2.50767078e-02  1.93377405e-01  5.00803143e-02 -1.01252161e-01
   1.30408990e+00]
 [-1.47555312e-02 -4.43227356e-03 -3.82985570e-03 -8.97485018e-03
  -7.74151742e-01]
 [ 3.78487229e-01  4.22741592e-01  3.85106802e-01  3.80600512e-01
   5.32994270e-02]
 [-2.12123208e-02  4.18190584e-02 -1.18197864e-02  4.18146439e-02
  -2.19162896e-01]
 [ 6.05584346e-02  4.09463188e-03 -7.36083984e-02 -6.99889362e-02
  -2.48183623e-01]
 [ 1.99286427e-04 -2.37807259e-02  1.23887486e-03 -2.80508213e-02
   3.82343858e-01]
 [ 2.20502215e-03 -9.32552665e-03 -5.64964395e-03  4.35690694e-02
  -4.69014615e-01]
 [ 2.08790302e-01  2.58232355e-01  2.41811335e-01  2.57518232e-01
   3.31581049e-02]
 [ 1.37207061e-02  3.59803960e-02  8.96790717e-03 -1.51660349e-02
   5.46821594e-01]
 [-1.04439603e-02  3.55340913e-02 -5.53294783e-03 -2.41312641e-03
  -4.49020326e-01]
 [ 3.44246849e-02 -7.26886373e-03 -1.01997247e-02 -8.09472927e-04
  -8.75983059e-01]
 [ 2.66855489e-03 -5.50844008e-04 -9.60494950e-03 -1.24006579e-02
   2.75123507e-01]
 [-3.39658320e-04 -1.52036047e-03  2.26515140e-02 -3.56490142e-03
   3.26378405e-01]
 [-3.51227224e-02 -3.69683765e-02 -4.39248793e-02 -5.75025976e-02
   2.08101422e-01]
 [-2.75565796e-02  1.64754838e-02  6.88600354e-03 -8.14276189e-03
  -8.34523559e-01]
 [-2.04929542e-02  2.27008876e-03  6.31368905e-03  3.59379873e-03
   3.50828499e-01]
 [ 3.52948233e-02  1.05752787e-02  5.41211143e-02 -1.63813848e-02
   4.31482136e-01]
 [-8.37164968e-02  9.77580994e-02 -1.16266720e-01 -4.47892733e-02
   1.22700311e-01]
 [ 6.15228079e-02 -1.31176829e-01  5.03597595e-02  1.54567748e-01
  -5.93171269e-02]
 [ 3.78380641e-02  2.11276174e-01 -1.85472757e-01  1.25119209e-01
  -1.58702135e-01]
 [ 1.21040307e-02 -1.65423173e-02 -1.71481539e-02  5.02756536e-02
   6.54219806e-01]
 [ 2.76897520e-01  1.02786727e-01 -1.03098467e-01 -7.10705668e-02
  -2.43298765e-02]
 [ 2.68487036e-01  2.53706217e-01  2.59579927e-01  2.65086889e-01
   1.04470007e-01]
 [-1.63062960e-01 -5.50793521e-02  1.07342772e-01 -4.68496094e-03
  -5.42276986e-02]
 [ 1.84501857e-02  7.85614550e-03 -1.40549466e-02 -4.30005416e-03
  -3.68869394e-01]
 [ 7.63795227e-02  1.37739172e-02  6.12942129e-02 -1.02019152e-02
   1.09378469e+00]
 [ 9.84142572e-02 -6.46779463e-02 -1.82274669e-01 -1.15012020e-01
  -1.74923196e-01]
 [-2.74684839e-02 -4.12807688e-02 -1.10183164e-01 -2.18324270e-02
  -3.28417599e-01]
 [ 9.09987185e-03  1.01079345e-02  7.08337408e-03  1.06236211e-03
  -9.96553957e-01]
 [-1.17234901e-01  3.35476138e-02 -2.06356049e-01 -1.32136554e-01
  -5.95614091e-02]
 [ 1.51797399e-01 -1.51536530e-02 -4.09349576e-02  9.61674303e-02
   3.69090065e-02]
 [-2.36623045e-02 -8.95589143e-02  4.51581180e-02  1.14687487e-01
   1.61705747e-01]
 [ 2.56277025e-02  1.19349267e-03  3.18282004e-03 -9.80233657e-04
  -1.32278728e+00]
 [ 1.05930464e-02 -5.91804320e-03  7.16020772e-03  2.88691688e-02
  -4.74661589e-01]
 [-3.17361462e-03 -2.19960567e-02 -1.06991660e-02  4.82265577e-02
   1.96088448e-01]
 [ 4.80265021e-02 -2.41996441e-02  6.08884469e-02 -6.51632845e-02
   2.24290013e-01]
 [ 3.90822053e-01  3.04356277e-01  3.12703729e-01  3.39057386e-01
   1.01665109e-01]
 [ 1.77619547e-01  1.31699502e-01  1.77490339e-01  1.56231537e-01
  -5.00425603e-03]
 [ 2.45041952e-01  2.66661853e-01  2.96304941e-01  2.21567884e-01
   7.10815340e-02]
 [ 2.79993534e-01 -2.35655338e-01 -2.84601957e-01 -1.62724614e-01
   9.22149629e-05]
 [-2.34787818e-03  1.54984146e-02 -5.71450219e-03  4.63696755e-03
   1.14259934e+00]
 [ 1.56022226e-02  1.10539822e-02  1.43716787e-03 -4.40574847e-02
   1.32537854e+00]
 [ 1.22063860e-01  7.65153766e-02  1.01252288e-01  2.88485549e-02
   1.29002139e-01]
 [-1.14975661e-01 -4.63769436e-02  9.82176587e-02  1.53372943e-01
   4.09560874e-02]
 [ 1.16924897e-01  1.10262863e-01  1.17826328e-01  1.05036065e-01
   2.77147554e-02]
 [-2.52877735e-02 -2.28196066e-02  1.28689051e-01 -9.02266651e-02
  -2.86417571e-03]
 [ 1.56115875e-01 -1.02073856e-01  4.30111438e-02  1.96280092e-01
   9.71645059e-04]
 [ 3.03072222e-02  6.08625216e-03 -3.48981842e-02 -1.37558766e-02
   1.21400094e+00]
 [ 1.77612640e-02  2.67985233e-05  7.26146298e-03  1.14647085e-02
   7.27177978e-01]
 [ 1.08081140e-01 -1.44741461e-01 -1.17916904e-01  3.71080004e-02
  -2.53860410e-02]
 [ 6.55086711e-02  1.71071097e-01 -2.57045895e-01 -1.45119742e-01
   1.85263101e-02]
 [-3.24131511e-02  1.59051479e-03 -2.08623707e-02 -1.01423329e-02
  -1.11899996e+00]
 [-3.99968699e-02 -6.94076903e-03 -4.07020524e-02  2.93556247e-02
   6.74492359e-01]
 [ 1.73861813e-02  2.09682360e-02 -1.62485272e-01  4.89797480e-02
  -1.88054070e-01]
 [-1.95212975e-01 -6.63565472e-02 -1.98003545e-01 -5.05446754e-02
  -8.38662386e-02]
 [ 2.05657817e-02  1.02904523e-02  2.34703030e-02 -5.05378982e-03
  -1.00661874e+00]
 [-1.31121412e-01 -1.41662294e-02 -1.73403993e-01  8.78460035e-02
   2.68880516e-01]]: "
2018-04-24 14:12:14,494 (dqn_agent.py:162) DEBUG: "var_name: q/fully_connected_1/biases:0, var_value [ 0.3226075   0.4083015   0.31867546  0.32637635 -0.10090829]: "
2018-04-24 14:12:14,500 (dqn_agent.py:162) DEBUG: "var_name: target_q/Conv/weights:0, var_value [[[[-0.3842626   0.42710143 -0.48237562  0.7081746  -0.03765922
    -0.14166075 -0.6734322   0.8003955 ]
   [ 0.68954164 -0.01749362  0.72579926 -0.66363627  0.4244573
    -0.5211806  -0.7130368   0.69630194]]]]: "
2018-04-24 14:12:14,506 (dqn_agent.py:162) DEBUG: "var_name: target_q/Conv/biases:0, var_value [ 0.02014444 -0.16396086 -0.08457343 -0.07023297 -0.15090875  0.
  0.         -0.2088292 ]: "
2018-04-24 14:12:14,519 (dqn_agent.py:162) DEBUG: "var_name: target_q/Conv_1/weights:0, var_value [[[[ 1.57908976e-01  2.36263484e-01  1.65418655e-01  1.66984014e-02
     3.10316443e-01 -1.57538280e-01  1.48687407e-01  7.96430707e-02
     1.41747728e-01  7.87209272e-02 -5.90410717e-02  2.69192815e-01]
   [-4.75146830e-01  3.60437125e-01 -4.73256074e-02 -1.14829009e-02
    -2.61039317e-01 -1.11773521e-01 -1.54554939e+00  3.40772241e-01
    -1.08163707e-01 -1.97432563e-02 -1.08842731e-01 -4.61841673e-01]
   [ 2.70208478e-01  3.19436401e-01  1.10185698e-01 -1.55875877e-01
     8.88662338e-02 -2.32913256e-01  1.81336895e-01  5.41864848e-03
     7.87939783e-03 -4.39741127e-02 -2.80115157e-01  2.33532086e-01]
   [-8.24942961e-02 -6.80780932e-02  1.83544606e-01  2.38941908e-02
    -4.09093350e-01 -5.85916162e-01 -8.23739707e-01  1.90047368e-01
     1.90484256e-01 -3.60872708e-02  2.75956213e-01 -7.58629143e-01]
   [-5.99671751e-02  3.26964170e-01  3.32019001e-01  2.29968899e-03
     1.35696650e-01  1.10621769e-02 -1.23155564e-01  1.59698233e-01
    -1.33730128e-01  1.65672228e-02 -2.21143126e-01 -1.74409539e-01]
   [-1.16928458e-01 -1.69084474e-01  1.18552178e-01 -2.28864104e-02
    -1.73668787e-01  1.01377904e-01  1.59139305e-01  1.17340088e-02
    -1.37613475e-01 -3.11780572e-02 -1.55784696e-01 -2.68583000e-03]
   [-1.71064526e-01  1.66021168e-01  2.53989995e-02 -3.74134630e-02
     1.49930209e-01 -1.79436862e-01 -2.16339976e-02 -8.60414878e-02
     4.01067734e-03  1.64670467e-01 -9.94056985e-02 -1.33151576e-01]
   [-3.96222830e-01  4.18078601e-01 -4.08231951e-02  1.54351085e-01
    -2.58399755e-01 -3.74619424e-01 -6.96594536e-01  2.42288709e-01
    -4.08265879e-03 -1.51941583e-01 -2.87518322e-01 -4.04192805e-01]]

  [[-4.81365802e-04  2.49943599e-01 -5.47469705e-02 -2.30093434e-01
    -3.07046086e-01  2.65622288e-01  2.03240976e-01 -1.00699283e-01
    -1.69326454e-01  2.07257345e-01  4.39728908e-02  1.22928351e-01]
   [-2.36170888e-01 -3.85423899e-01 -1.71759933e-01  2.82904536e-01
     1.10593684e-01 -2.90672719e-01 -2.55927742e-01 -1.76460683e-01
     1.57845274e-01 -2.33886037e-02 -4.77025300e-01  5.46736992e-04]
   [ 2.27879971e-01  2.97392249e-01 -1.62640437e-01 -3.78578603e-01
    -1.50147574e-02  2.24969819e-01 -1.14409484e-01 -6.33420423e-02
    -1.73121333e-01  3.24507236e-01  9.10809264e-02  6.21810928e-02]
   [ 2.77494699e-01 -8.29813331e-02  2.26114467e-02  7.74464682e-02
     5.40722191e-01 -2.47540727e-01 -4.80735660e-01 -3.25099379e-01
     1.23093240e-01 -3.70132886e-02 -2.85582095e-01 -4.37713891e-01]
   [-2.29448423e-01 -2.63097435e-02 -2.72558272e-01 -1.08655781e-01
    -2.41464958e-01  1.02681980e-01 -5.77781275e-02 -1.27126481e-02
    -9.10791159e-02  2.18710765e-01 -5.19911125e-02 -1.48143291e-01]
   [-1.92211717e-02  1.19614035e-01  5.93356639e-02  6.83401227e-02
     7.57441223e-02 -1.78623825e-01 -1.40400827e-02  1.41271919e-01
    -9.76257548e-02  1.73211545e-01  9.61068869e-02 -1.10053718e-02]
   [-5.33091426e-02  1.31113112e-01 -4.51277643e-02 -1.15980051e-01
    -1.56379491e-02 -1.45686328e-01 -1.74439117e-01 -7.03044683e-02
     1.49843365e-01  5.55234253e-02  3.97729576e-02 -5.61143756e-02]
   [-3.86347562e-01 -5.64558804e-01 -1.67495802e-01  2.97553092e-01
     1.58062831e-01 -2.05908164e-01 -1.23315090e-02  4.68632095e-02
    -2.94786319e-02 -5.43889180e-02 -3.06776047e-01 -9.86050218e-02]]

  [[ 8.07159990e-02 -3.90751690e-01  1.59415185e-01  1.70167223e-01
    -9.30788219e-01 -3.45105171e-01 -6.81052282e-02  3.02871108e-01
     1.59005687e-01  1.01407938e-01  1.03068285e-01 -2.64251262e-01]
   [-6.32866696e-02  2.23434865e-01 -6.22349858e-01  2.63307065e-01
     1.09729804e-01  6.18282408e-02  2.49899969e-01 -5.60025312e-03
    -5.01511753e-01 -8.86207879e-01 -4.65429336e-01  2.88766831e-01]
   [-8.38905573e-02 -3.47698033e-01  2.66504884e-01  1.18181042e-01
    -7.11641967e-01 -1.75778285e-01 -2.32443169e-01  1.76154137e-01
    -1.28819570e-01  4.10540968e-01  8.14414248e-02 -2.42754057e-01]
   [-4.56881166e-01  1.61951751e-01 -2.26481333e-01 -2.14851081e-01
     4.64590549e-01  5.53538382e-01  3.44046026e-01 -4.90610510e-01
    -1.86957240e-01  8.51534382e-02  2.68984258e-01  1.64116785e-01]
   [-1.16558298e-01 -2.54598141e-01 -2.02699631e-01  1.37550578e-01
    -8.40490997e-01 -2.09543005e-01 -6.87994510e-02  3.63003433e-01
    -1.80544704e-02  5.63965924e-02 -2.04739556e-01  7.35405982e-02]
   [ 1.08801365e-01  3.61566246e-03 -1.01989627e-01 -9.79241878e-02
    -1.51516706e-01  1.32251561e-01  2.70187110e-02  1.21899873e-01
     1.59162372e-01 -8.13324675e-02  4.81122136e-02  1.86986625e-02]
   [-7.18268529e-02  3.86774540e-02  1.82534009e-01  1.39233470e-02
     1.60867900e-01  7.48507380e-02 -1.01700947e-01 -3.06328982e-02
     1.37778223e-01 -1.66307405e-01 -1.19131207e-01 -1.12202726e-01]
   [-1.21806897e-01  1.31165743e-01 -3.51859421e-01  2.93061972e-01
    -1.64214566e-01  1.56165928e-01  1.36923313e-01  3.13896000e-01
    -5.69873273e-01 -6.53537989e-01 -4.04817134e-01  4.67037439e-01]]]


 [[[-1.10831246e-01  4.06544328e-01 -2.78275669e-01  5.89123294e-02
    -2.64746219e-01 -1.72582537e-01 -3.94065231e-01 -1.22421697e-01
    -5.22435188e-01 -3.13763678e-01  6.14634454e-01  3.57794017e-01]
   [ 2.46257231e-01 -2.86270261e-01  1.34583876e-01 -9.31663960e-02
     3.13513339e-01  2.58915693e-01 -1.20839663e-02 -1.53334229e-03
     1.19409924e-02 -3.03014547e-01  1.47201136e-01  2.82972544e-01]
   [-1.73776329e-01  4.44217622e-01 -8.40070769e-02 -9.42963362e-02
    -1.05975233e-01  1.71118721e-01 -2.07444191e-01 -2.10384458e-01
    -1.13536513e+00 -6.00215085e-02  6.36085331e-01  8.77584741e-02]
   [ 3.55788946e-01 -3.91922683e-01  4.43024421e-03  2.20266983e-01
     6.15502417e-01  5.79290211e-01  1.28737241e-01  3.02889049e-01
     3.20146501e-01  3.20414573e-01 -1.35631561e-01 -3.51263434e-01]
   [ 4.62412387e-02  3.66610229e-01 -3.91889829e-03 -2.44033448e-02
    -2.05416903e-01 -8.60904455e-02 -3.02614897e-01 -8.72692466e-02
    -1.12489796e+00 -2.12111339e-01  3.80114228e-01  3.13941479e-01]
   [-4.14004177e-02 -7.58840665e-02 -1.63450032e-01  8.28725696e-02
     5.11182845e-02  9.55280364e-02 -1.82228386e-02  1.80264413e-01
    -2.79470533e-02  1.19096726e-01  4.23515290e-02 -7.85641968e-02]
   [ 7.92072415e-02 -2.74528563e-03  6.48586750e-02 -3.33484560e-02
     6.62521720e-02  1.33071750e-01 -1.74410746e-01  5.30443192e-02
    -9.45934281e-02 -1.14654280e-01 -1.38677761e-01  2.05377191e-02]
   [ 3.39709550e-01  1.45732924e-01 -1.99662581e-01 -9.62167978e-02
    -1.61879808e-02  7.49328732e-02 -1.13791876e-01  1.55823052e-01
    -4.69134718e-01 -4.78766292e-01  4.52500194e-01  4.36450273e-01]]

  [[-1.19477019e-01 -1.69855766e-02  2.40208060e-02 -1.07246213e-01
    -4.46797311e-02 -3.60024184e-01  1.27797604e-01 -1.04783550e-01
    -1.47091985e-01 -2.19602793e-01  4.05334979e-01 -1.43424109e-01]
   [ 1.25179350e-01 -3.05135816e-01 -1.00171492e-01 -2.85954535e-01
     5.49582206e-02  1.63014624e-02  2.39172727e-01  3.92014980e-02
    -3.88496876e-01  5.69873810e-01 -1.89716354e-01  2.52970289e-02]
   [-2.62796611e-01 -1.41272351e-01 -3.35058719e-01 -3.53101701e-01
     1.51863582e-02 -5.03266871e-01  2.70671815e-01  7.99473897e-02
    -1.01253837e-01  1.48031916e-02  5.01799524e-01 -1.02065891e-01]
   [ 4.14337218e-01 -1.40646413e-01 -2.05751732e-01  1.24890648e-01
     8.32057819e-02  1.66901171e-01  5.52104712e-01  6.81142211e-02
    -3.18518966e-01  5.70630968e-01 -2.47131642e-02  3.33200574e-01]
   [ 6.70211464e-02 -3.95243280e-02 -1.68939441e-01 -3.23845088e-01
     2.37793878e-01 -4.73449469e-01  6.61735758e-02  9.40135643e-02
    -2.42645964e-01 -2.68569924e-02  2.59855300e-01 -4.20236826e-01]
   [ 1.71099395e-01  1.09755486e-01  1.38165027e-01 -5.69903851e-03
     1.75003201e-01  1.44286782e-01 -5.61501086e-02 -2.53363997e-02
     5.38746715e-02 -7.73933530e-02 -1.31253272e-01  1.33172303e-01]
   [-4.72038090e-02 -3.75347286e-02  3.48986536e-02  2.25902796e-02
    -1.46622568e-01  9.67561603e-02  8.91182125e-02  1.71213776e-01
     4.89567220e-02 -2.77263671e-02 -8.67928043e-02 -1.29502416e-02]
   [ 2.18246818e-01 -1.84651613e-01 -6.65014684e-02 -2.60886580e-01
    -4.97232340e-02 -1.35398373e-01  4.38994855e-01 -3.72385457e-02
    -3.45756888e-01  5.14636099e-01  1.35959476e-01 -2.09816843e-01]]

  [[-1.58079311e-01 -7.56787583e-02 -1.68546528e-01 -2.70298421e-02
     1.31556034e-01 -8.58368278e-02 -2.53347725e-01  2.20081866e-01
    -3.86355132e-01  2.87495166e-01 -3.26335371e-01  1.23117574e-01]
   [ 2.69421726e-01  3.21531504e-01 -1.16314530e-01  2.24157304e-01
     8.51133764e-02  2.64326781e-02 -4.24365140e-02 -1.46445587e-01
     5.18641286e-02  1.16175950e-01 -8.25552642e-02  3.62323821e-02]
   [-2.65971154e-01  2.46953741e-02  2.05064900e-02 -1.21199377e-01
     1.63869381e-01  1.76464841e-01 -3.59445214e-01  1.98104769e-01
    -5.82658470e-01  1.49706587e-01 -2.36881495e-01 -2.44553760e-02]
   [ 9.56592739e-01  2.65036970e-01  1.12604156e-01 -2.22745031e-01
     3.43698114e-01  4.05874342e-01  1.75246626e-01  4.03815582e-02
     4.11952168e-01 -2.24570632e-01  2.31736019e-01  1.32455304e-01]
   [-2.75441080e-01 -1.00864500e-01 -8.33010003e-02 -3.06578912e-02
     1.84164435e-01 -1.41497791e-01 -2.98967183e-01 -6.56041428e-02
    -1.66233048e-01  9.82314423e-02 -3.20792049e-01 -8.75578076e-02]
   [-1.57247439e-01 -1.43974036e-01 -1.11414373e-01  1.74075186e-01
     9.05815363e-02  6.04171902e-02 -7.25063831e-02  1.98715031e-02
     1.43994957e-01 -3.13255191e-02  8.25239122e-02 -1.81371436e-01]
   [ 7.57984817e-02  9.56378281e-02 -3.11301649e-02 -1.04971677e-02
    -3.86978686e-02 -1.40289605e-01  5.17287850e-02  1.66545480e-01
    -2.83560157e-02 -2.57818401e-02  1.71208739e-01 -6.82893768e-02]
   [ 1.32579818e-01  2.69180059e-01 -1.62481934e-01  2.26246431e-01
     3.73023868e-01 -8.84454772e-02 -1.84525579e-01  7.32530728e-02
    -1.31974727e-01  7.15410858e-02 -6.50875792e-02  1.61100522e-01]]]


 [[[-7.15327263e-02  1.91651165e-01  6.46197140e-01  7.00611174e-02
    -7.84784332e-02  5.75795397e-02 -2.38748983e-01  1.22987449e-01
    -9.35315788e-02  3.17766398e-01 -2.57059604e-01 -5.74876666e-02]
   [ 2.58538425e-01  4.55794185e-02 -7.70657808e-02  2.32896894e-01
     2.26833567e-01  3.12318206e-01  1.90086424e-01  3.33856583e-01
    -3.01765829e-01 -2.50239926e-03  4.49960567e-02  8.56213793e-02]
   [-5.74929006e-02  1.47675052e-01  6.59429729e-01  2.49413356e-01
    -1.64370120e-01  3.07702750e-01 -9.26343426e-02 -1.20850414e-01
    -2.99005699e-03  1.65415227e-01 -2.62678385e-01 -3.99998814e-01]
   [ 1.12412699e-01 -8.81603733e-02 -6.83575153e-01 -4.22962829e-02
     1.59264386e-01  1.02377713e+00  6.13784432e-01  6.02126658e-01
     2.92520344e-01 -1.96503118e-01  7.06683670e-04  2.72037834e-01]
   [-2.72509251e-02  7.19174296e-02  4.59328920e-01  1.80149525e-01
    -6.07462525e-02  1.88597783e-01  1.75807122e-02 -2.05919836e-02
     2.50963931e-04  1.41671419e-01 -1.74153298e-01  1.11124869e-02]
   [-1.27826005e-01 -1.49427399e-01 -3.03150415e-02  7.10646808e-03
     1.14063621e-02 -5.39898574e-02  1.54998302e-02 -4.40126359e-02
     1.58390731e-01  2.54238993e-02  4.43068147e-02  1.03162736e-01]
   [-4.40144241e-02 -8.69340077e-02 -4.61161435e-02 -1.58744410e-01
    -2.08557397e-02  1.25849217e-01 -7.77893811e-02  7.65181184e-02
     1.06382817e-02  1.45737827e-01 -1.70219988e-02 -2.56664008e-02]
   [ 7.30122030e-02  5.15643507e-02  3.86561513e-01  3.42454046e-01
     1.78751603e-01  3.17977548e-01  2.95535058e-01  2.71570887e-02
    -3.26570332e-01  1.62645295e-01 -1.92404538e-01  1.07023790e-01]]

  [[ 1.90208510e-01 -5.48228547e-02  1.98443457e-01  4.55673710e-02
    -2.05473259e-01 -1.24985248e-01  1.90139115e-01  2.02983931e-01
     4.47370708e-01 -6.01485223e-02 -1.77838162e-01  1.45341352e-01]
   [-8.84884149e-02  4.43429090e-02  1.05852164e-01 -4.73271534e-02
     3.92974652e-02 -1.96201548e-01  2.40134597e-01 -7.55818337e-02
     4.25930351e-01  1.13245048e-01 -1.73143923e-01  1.38743401e-01]
   [ 1.89319581e-01  2.96436972e-03  7.18576163e-02 -1.56092137e-01
    -1.36328503e-01 -4.07687187e-01  6.22976869e-02  4.35390435e-02
     5.17005801e-01 -1.38652012e-01 -2.29008690e-01 -7.95988068e-02]
   [ 4.29514527e-01 -1.48259133e-01  1.45888822e-02 -2.74065942e-01
    -1.24932110e-01 -4.14965063e-01 -7.07687855e-01  2.17434123e-01
     5.41878752e-02 -1.05230138e-01  1.67301103e-01 -3.81813377e-01]
   [-9.54849944e-02  1.12577006e-01  7.53609240e-02  1.40903324e-01
    -5.84919974e-02 -1.72238991e-01  3.49680513e-01 -1.24528944e-01
     3.24048936e-01  1.32916808e-01 -6.29770905e-02  2.24303812e-01]
   [-1.07972175e-02 -7.06320629e-02  4.71407175e-03  1.69853061e-01
    -1.48884892e-01  1.18483007e-01  1.67882472e-01 -1.26603708e-01
    -1.32022351e-02  1.40404075e-01  1.60586476e-01  6.81694150e-02]
   [-1.07416615e-01 -2.72439420e-02 -8.22232962e-02  8.92572403e-02
     9.25876498e-02  6.23116493e-02  4.98044044e-02 -1.37133390e-01
    -1.32860631e-01  1.78674817e-01 -5.09697199e-02 -8.95811617e-03]
   [-2.24528108e-02 -1.26325697e-01  1.70466620e-02 -2.13208601e-01
     2.80158278e-02 -1.77380994e-01  2.39674989e-02  4.48514447e-02
     5.11303902e-01  2.07002331e-02  3.51129076e-03  1.81627106e-02]]

  [[-9.28699225e-02 -3.64002138e-02  4.03782934e-01  2.57309586e-01
     8.02107304e-02 -2.22658068e-01  1.55284435e-01 -1.44406855e-01
     2.19841331e-01  1.12524018e-01  6.33226633e-02 -6.17507659e-02]
   [-1.25230074e-01  2.55384475e-01 -1.07362561e-01 -2.12351844e-01
     3.91005963e-01 -1.97019409e-02 -6.18963540e-02 -8.47284943e-02
     2.93940932e-01  1.92469601e-02 -3.42755288e-01  3.07034925e-02]
   [-1.22168943e-01 -1.58553600e-01  1.69391632e-01  7.97242969e-02
     4.54854965e-02  3.90238222e-03  3.37346047e-02 -5.77174388e-02
     4.81753536e-02 -1.15558408e-01 -5.14879599e-02  1.32555038e-01]
   [ 1.05236016e-01  6.76941812e-01 -4.01293874e-01 -5.76217473e-01
     4.80776370e-01 -3.00975591e-01 -6.55085504e-01  7.66352937e-02
     6.54669046e-01 -2.05003470e-01 -4.11959380e-01  1.20584302e-01]
   [-4.05460745e-02  9.47164968e-02  2.93236285e-01  2.38583580e-01
     2.87667036e-01 -4.74400446e-02  2.80839145e-01 -1.77495599e-01
     2.26608217e-02  1.87707558e-01  1.13622673e-01 -5.95980436e-02]
   [-5.41606992e-02 -9.23794135e-02  6.00416213e-02 -1.34202182e-01
    -1.12752758e-01 -9.00207460e-02  1.42227948e-01  6.48628920e-02
     6.48780465e-02  3.62690389e-02  1.16524458e-01 -1.05321914e-01]
   [-6.08861223e-02 -2.42796093e-02 -7.98016414e-02 -2.09693015e-02
     5.30975908e-02 -9.63776410e-02  2.78834254e-02 -1.61698297e-01
     7.26583898e-02 -3.16943973e-02 -6.96819574e-02 -6.99496195e-02]
   [-3.04783821e-01  1.73953310e-01  2.09537998e-01  6.75738901e-02
     3.82591993e-01 -4.07799840e-01  1.82924703e-01 -1.66502431e-01
     2.60014921e-01  5.93364723e-02 -1.69355959e-01  1.73690841e-01]]]]: "
2018-04-24 14:12:14,525 (dqn_agent.py:162) DEBUG: "var_name: target_q/Conv_1/biases:0, var_value [ 0.10535506 -0.14567755  0.01438164 -0.03695296 -0.03497759  0.03599799
 -0.046046   -0.02084113 -0.00026308  0.00947261 -0.01138524 -0.01994674]: "
2018-04-24 14:12:14,533 (dqn_agent.py:162) DEBUG: "var_name: target_q/Conv_2/weights:0, var_value [[[[-2.41449669e-01 -5.34254722e-02  2.55125999e-01 ... -2.69252002e-01
     1.46744326e-01 -1.64181128e-01]
   [ 3.18062067e-01 -1.15118206e-01 -2.17820838e-01 ... -3.29854101e-01
    -3.92338514e-01 -2.65229523e-01]
   [-5.22299826e-01 -1.89754181e-02  1.51664674e-01 ...  9.65580866e-02
    -1.75353527e-01 -9.42160115e-02]
   ...
   [-1.99625611e-01  4.32765894e-02 -2.35206008e-01 ... -9.32345837e-02
     9.39134210e-02 -5.16004823e-02]
   [ 3.13908219e-01  2.93489009e-01 -2.10851327e-01 ... -6.10644341e-01
     3.49426359e-01 -4.78882790e-01]
   [ 2.39711061e-01  1.50681445e-02  1.61193684e-01 ... -2.73865700e-01
     4.32406425e-01 -4.51224983e-01]]

  [[-1.33026466e-01 -1.26625255e-01 -1.98688254e-01 ... -9.74047601e-01
    -1.87634528e-01  1.84524074e-01]
   [ 2.83999264e-01  2.26553828e-02 -3.54044765e-01 ... -6.46213116e-03
    -2.11558178e-01  2.04273850e-01]
   [-3.70430276e-02  1.96939632e-01  3.95578384e-01 ...  5.66437878e-02
    -1.21716082e-01  1.06804423e-01]
   ...
   [ 2.66831547e-01  2.59639800e-01  3.08012962e-01 ... -1.44180447e-01
    -1.77539065e-01 -2.06214428e-01]
   [-1.85881168e-01 -4.68818955e-02 -1.93581104e-01 ... -2.04702124e-01
    -2.13256478e-01 -3.69633846e-02]
   [-7.06315711e-02  1.52284265e-01 -1.02523685e-01 ... -2.39206836e-01
    -1.99740261e-01  8.01080838e-02]]

  [[ 5.15035391e-01 -1.32619287e-03 -4.36251193e-01 ...  3.55067588e-02
    -2.86883861e-01  3.55606645e-01]
   [ 1.64627984e-01  3.28141272e-01  2.96485028e-03 ...  1.18057117e-01
     1.70026034e-01  3.83506157e-02]
   [ 2.73051471e-01  9.11929905e-02 -1.23570286e-01 ... -4.55043502e-02
    -2.90703207e-01  2.29321286e-01]
   ...
   [-3.71570349e-01 -4.55673128e-01  9.25039649e-02 ...  3.10655266e-01
    -4.70931619e-01  9.37193856e-02]
   [ 1.93520069e-01  1.41828761e-01 -2.51524955e-01 ...  1.39179960e-01
    -7.54920417e-04  2.29033753e-01]
   [-6.83036670e-02 -5.53677864e-02  2.00436831e-01 ... -3.11326742e-01
    -7.01230243e-02 -2.27393553e-01]]]


 [[[-1.13352910e-01 -4.93823849e-02 -1.17840301e-02 ...  1.84063613e-01
     5.83508946e-02  5.41840903e-02]
   [ 5.32101631e-01  1.57478377e-01  1.97748706e-01 ... -3.54088485e-01
     1.64472193e-01 -3.66036624e-01]
   [ 3.19313318e-01  3.11722700e-03 -6.30381852e-02 ...  4.19514596e-01
     1.83600709e-01  9.88867134e-02]
   ...
   [ 1.41395271e-01  1.70138389e-01 -2.26125807e-01 ...  1.45689189e-01
    -1.10882245e-01 -1.23609863e-01]
   [-4.36667830e-01 -5.07290125e-01 -8.32695886e-02 ... -1.28299683e-01
    -4.14282441e-01  1.80860355e-01]
   [ 5.84287290e-03  6.24661222e-02  1.24327779e-01 ...  1.07817523e-01
     3.48861925e-02  1.30702853e-01]]

  [[ 5.29435463e-02 -2.28952482e-01 -2.50350654e-01 ...  5.89164197e-02
    -2.03094691e-01 -1.79305881e-01]
   [ 1.51476309e-01 -1.45623565e-01  1.17405415e-01 ...  1.41527578e-01
    -1.62230343e-01  1.04582615e-01]
   [-2.75358055e-02 -2.52247639e-02  1.51612878e-01 ... -1.33030340e-01
     6.44853562e-02 -3.45877856e-01]
   ...
   [-5.23115575e-01 -2.05495376e-02  1.49583414e-01 ... -7.25965202e-02
     1.04104556e-01 -1.42992765e-01]
   [-4.89017069e-02 -1.67753696e-01 -1.16551757e-01 ... -2.98356324e-01
    -5.53530753e-01 -1.19592041e-01]
   [-1.04106680e-01 -4.20228571e-01 -9.01095197e-02 ...  4.57712501e-01
    -1.09294690e-01 -5.93658499e-02]]

  [[ 1.01441694e-02  8.15739483e-02  4.32100385e-01 ...  1.78149238e-01
     1.10713609e-01  2.55182028e-01]
   [ 2.21322179e-01  3.03741712e-02  2.27362514e-01 ...  6.87184632e-02
     1.87759563e-01  2.78439760e-01]
   [ 1.69676274e-01 -8.48821700e-02  4.72275823e-01 ... -1.00616179e-01
    -3.12018320e-02 -1.48578644e-01]
   ...
   [ 2.51435697e-01 -1.36775091e-01  3.42942446e-01 ... -6.30166888e-01
     1.67863965e-01  3.05713028e-01]
   [-3.74233127e-01  4.31891799e-01  4.12264407e-01 ... -5.34088671e-01
    -8.62760171e-02  2.88886309e-01]
   [-2.76844949e-01 -1.19374216e-01 -3.03454727e-01 ...  1.64526358e-01
    -5.92421293e-02 -2.24565137e-02]]]


 [[[-1.04004666e-01 -2.06602126e-01 -3.31472129e-01 ...  4.92547989e-01
    -8.65056440e-02  4.35116254e-02]
   [-2.30724350e-01  3.26067954e-01  3.11782211e-01 ...  9.29225683e-02
    -2.47843415e-01  2.89765865e-01]
   [ 1.21429926e-02 -2.04406664e-01 -4.54302758e-01 ...  9.83411446e-02
     1.85889214e-01 -4.05271739e-01]
   ...
   [ 2.71381974e-01  7.06129149e-02 -3.89743179e-01 ...  4.25456434e-01
     1.79809779e-01 -2.13114783e-01]
   [ 3.85508477e-03  1.28412619e-01 -4.03910339e-01 ...  5.76406009e-02
    -3.60745579e-01  7.25495890e-02]
   [-2.02729613e-01 -9.56437588e-02  4.22800303e-01 ... -8.59438479e-02
    -8.12424272e-02  1.41532451e-01]]

  [[-1.13986187e-01  2.11814567e-01 -6.38982505e-02 ...  5.85329592e-01
    -1.03516385e-01 -8.57791975e-02]
   [-2.36424044e-01 -3.72458138e-02 -1.55087903e-01 ... -2.91638434e-01
    -2.27965474e-01  4.92279232e-02]
   [ 8.06549340e-02 -4.24761266e-01 -1.15729570e-01 ... -5.31054854e-01
     6.68823302e-01  2.78068274e-01]
   ...
   [-9.66653004e-02 -2.67881870e-01  1.85834616e-02 ... -1.61479469e-02
     3.83571893e-01 -1.51251070e-02]
   [-1.02213061e+00 -8.49537179e-03  1.81891561e-01 ... -4.72231269e-01
    -1.21338621e-01  9.14643556e-02]
   [ 1.30873531e-01 -1.27570033e-01  3.27024749e-03 ... -1.03149697e-01
     1.04601018e-01 -5.49281612e-02]]

  [[ 2.54847407e-01  2.26196617e-01 -1.58632189e-01 ...  2.00050205e-01
    -5.06856799e-01 -1.81164950e-01]
   [-1.92876756e-01 -1.04030281e-01  4.02533561e-01 ... -2.60307193e-01
     1.81405261e-01  2.12030597e-02]
   [-1.61670640e-01 -6.35528147e-01  8.20311680e-02 ... -1.84406593e-01
    -5.03129184e-01 -1.32312894e-01]
   ...
   [-4.56179947e-01 -1.06577270e-01  3.37775759e-02 ...  2.52632797e-01
    -4.07951139e-03  5.92708737e-02]
   [-6.83571100e-01  2.91390657e-01  4.16576743e-01 ... -3.96494091e-01
    -6.72485173e-01 -1.42043337e-01]
   [ 1.25826284e-01  5.65347522e-02  2.24754781e-01 ...  1.30325392e-01
     1.94313433e-02  2.74663746e-01]]]]: "
2018-04-24 14:12:14,538 (dqn_agent.py:162) DEBUG: "var_name: target_q/Conv_2/biases:0, var_value [ 0.11895626  0.24126464  0.04300895  0.22310545 -0.2430591  -0.22689237
  0.45065308  0.08348394  0.3702053   0.13443358  0.43161657  0.33025563
  0.20172037  0.1819763   0.1345724  -0.07143605]: "
2018-04-24 14:12:14,543 (dqn_agent.py:162) DEBUG: "var_name: target_q/fully_connected/weights:0, var_value [[ 0.23050644  0.01994356 -0.36850002 ...  0.06404281 -0.28993225
  -0.24448916]
 [ 0.06606911  0.0396628   0.02745071 ... -0.02488617  0.1913353
  -0.5438785 ]
 [-0.04089682  0.05491148  0.07513998 ... -0.09645336  0.17715548
  -0.53310746]
 ...
 [-0.8391496  -0.07790687 -0.2952602  ... -0.0121758  -0.31202227
   0.03637012]
 [-0.20549847  0.08081612  0.15979487 ... -0.0629594   0.27244717
  -0.0375578 ]
 [ 0.13435096  0.00546949  0.02023502 ... -0.1010722  -1.3518869
   0.5769472 ]]: "
2018-04-24 14:12:14,550 (dqn_agent.py:162) DEBUG: "var_name: target_q/fully_connected/biases:0, var_value [ 0.36725098 -0.00957042 -0.53188896 -0.02027706 -0.2297823   0.5917957
  0.27515936 -0.01725937  0.14079165 -0.27893326 -0.01430109 -0.01365287
  0.40094674  0.458554   -0.30088088 -0.3032073   0.30065247  0.113043
 -0.06180681  0.08631813  0.03617646  0.5287037  -0.55937034 -0.3435705
 -0.5580117  -0.01647493  0.15452044  0.24080233  0.02292323 -0.22531638
  0.28286642 -0.01600578 -0.56755364  0.32128954 -0.20352238 -0.01077665
 -0.0988888   0.2779302   0.22951114 -0.11411165 -0.33250073  0.44001615
  0.32495728  0.08392926 -0.00632227  0.15183495  0.31513324 -0.48193955
 -0.3429818   0.27584788 -0.02309261  0.14946471  0.38567027 -0.2057611
 -0.15081486 -0.5855758  -0.02096778 -0.5055112  -0.11600184 -0.34131455
  0.23414946  0.07400464  0.19944489 -0.0646525  -0.29653051 -0.00985826
 -0.02057208  0.00603723 -0.00992004 -0.28512323  0.1033206  -0.13445479
  0.14007077  0.54190993  0.16918707  0.05420721 -0.05212287  0.2807009
  0.4969421  -0.23901068  0.28754887 -0.09464515 -0.5021092  -0.3413578
 -0.2848077  -0.04237092 -0.582475   -0.3953621  -0.01612202 -0.01821942
 -0.01286166 -0.51564294 -0.04988134  0.5129274  -0.0167745   0.38166595
 -0.12671225 -0.01711439  0.15363972  0.2643233  -0.18248458  0.06745163
 -0.01670748 -0.00742324  0.05437957 -0.41286448 -0.40868005  0.5687168
  0.23134357  0.39478546 -0.08628965 -0.279669   -0.24108958 -0.01603
 -0.12005944  0.10043397  0.1603841  -0.02543154 -0.05282281 -0.43251607
 -0.13380371 -0.0285012   0.04026359 -0.09882317 -0.00938291 -0.02169454
 -0.04231279 -0.4575058 ]: "
2018-04-24 14:12:14,559 (dqn_agent.py:162) DEBUG: "var_name: target_q/fully_connected_1/weights:0, var_value [[ 1.65600684e-02  8.21991544e-03  1.19407875e-02 -1.52987232e-02
  -1.48181391e+00]
 [ 1.90950297e-02  1.18576542e-01  5.77700324e-02 -1.52934566e-01
   1.37765929e-01]
 [-1.68544799e-02 -1.64275058e-02  3.20471413e-02  2.15784740e-02
   3.94471228e-01]
 [-1.67746797e-01 -1.40990898e-01  8.08826461e-02 -5.10361157e-02
   1.31694332e-01]
 [-3.57777983e-01 -3.44890684e-01  1.45926863e-01 -2.96965718e-01
  -1.70494378e-01]
 [ 3.14272612e-01  2.80956894e-01  3.24050725e-01  3.48681182e-01
   8.38135928e-02]
 [ 3.63558740e-03  4.17484064e-03  2.06496101e-02  1.50509449e-02
  -3.95924002e-01]
 [-4.26310971e-02 -1.02004692e-01  7.76893720e-02  1.05672993e-01
   1.27461761e-01]
 [ 1.46346524e-01  1.13069788e-02  1.73250854e-01  4.07079933e-03
   6.00470193e-02]
 [ 4.72487230e-03  1.71657130e-02 -2.25282796e-02  7.16674142e-03
   1.29911864e+00]
 [ 1.70211568e-02  1.03218667e-02  1.02940038e-01 -1.83181137e-01
   3.73155414e-03]
 [ 1.14322685e-01  5.62420972e-02  1.12488516e-01 -2.76166218e-04
   1.53054953e-01]
 [ 2.46371955e-01  2.42954955e-01  2.49446303e-01  2.51843601e-01
   5.14376201e-02]
 [ 2.74960965e-01  3.84722114e-01  2.84657419e-01  2.33405679e-01
   5.81559874e-02]
 [ 6.26749359e-03  4.03657556e-02  1.23744868e-02  1.67631656e-02
   1.27137327e+00]
 [-1.26388475e-01 -2.44682074e-01 -5.14499024e-02 -2.72055149e-01
  -2.01021042e-03]
 [ 3.16045843e-02  8.85612238e-03 -2.87955860e-03 -1.16332136e-02
  -2.51959026e-01]
 [ 1.44887343e-02 -1.23441787e-02 -8.41094647e-03 -1.44999195e-02
  -4.26376224e-01]
 [-6.30260333e-02 -1.11343838e-01  7.03793541e-02  7.07648844e-02
  -1.50890741e-02]
 [ 1.34927509e-02 -1.11334100e-02  1.59910321e-02  2.39011087e-02
  -8.58882606e-01]
 [ 9.20458212e-02  1.18108295e-01  4.68541235e-02  8.27357918e-02
   3.63065511e-01]
 [ 8.31320230e-03  2.23744288e-02  7.38444505e-03  5.30262664e-03
  -4.96045530e-01]
 [-2.89638024e-02 -7.45551195e-03 -1.92802120e-02  1.34054665e-03
   2.50998646e-01]
 [-1.18887685e-02  8.84941686e-03  1.46514801e-02 -7.62394071e-03
   6.30913734e-01]
 [-4.82296245e-03  2.55959178e-03 -2.57881787e-02 -3.60355005e-02
   6.27395391e-01]
 [ 5.19447066e-02  1.73286512e-01  1.32516026e-01  3.42205763e-02
  -7.46701956e-02]
 [-7.37826992e-03  2.28777733e-02  8.99024960e-03 -1.52714392e-02
  -7.71260321e-01]
 [ 1.45536199e-01  2.41292372e-01  1.89142779e-01  2.17621639e-01
   8.21681097e-02]
 [ 1.61166996e-01 -3.15095596e-02 -9.59682092e-02 -3.66463773e-02
   2.36271191e-02]
 [-6.86036237e-03 -4.14890721e-02 -1.37462653e-02  2.69596744e-02
   7.08291292e-01]
 [ 1.85471550e-01  1.57760501e-01  1.60610586e-01  1.64407998e-01
   6.09930232e-02]
 [-1.78311735e-01  2.60802414e-02  1.01164736e-01 -1.24185853e-01
   5.65709807e-02]
 [ 2.80439761e-02 -5.15293293e-02 -5.45181558e-02  3.20061706e-02
   2.40598425e-01]
 [-4.91542742e-02 -2.78870799e-02  3.94679606e-02 -3.86412884e-03
  -3.09400111e-01]
 [-2.14788709e-02 -2.08785851e-03 -2.83991988e-03 -1.08700609e-02
   3.07833374e-01]
 [-1.47071078e-01  8.23467523e-02  1.52535066e-01 -9.68541950e-02
  -1.45330802e-01]
 [-4.79916530e-03  1.78459510e-02  1.50136894e-03 -1.56141864e-02
   1.59608793e+00]
 [ 1.10883657e-02 -1.45671545e-02 -2.03204509e-02  1.59661248e-02
  -4.28157926e-01]
 [ 2.09581971e-01  2.66255260e-01  2.54919678e-01  2.40095958e-01
   8.90041068e-02]
 [-5.73571622e-02 -3.00303120e-02 -1.68957021e-02  4.20386232e-02
  -4.38913405e-01]
 [-5.36596216e-03  1.25923278e-02 -2.02285070e-02 -6.35271845e-03
   5.56074560e-01]
 [-4.28210106e-03  1.87039119e-03  1.48715135e-02  1.52901243e-02
  -4.21740592e-01]
 [ 1.82804335e-02  8.08990211e-04  3.54676740e-03 -1.07891122e-02
  -5.95536470e-01]
 [ 7.04839602e-02  1.89517647e-01  1.38030320e-01  1.34698942e-01
   7.18456751e-04]
 [-9.47529301e-02  1.83694080e-01  9.80010852e-02  1.62337556e-01
   1.12958059e-01]
 [ 2.50934158e-02 -2.03973055e-02 -2.96321362e-02 -2.23056339e-02
  -4.28137153e-01]
 [ 3.32727611e-01  3.40673864e-01  3.23047638e-01  3.32426995e-01
   1.93948999e-01]
 [-7.35085785e-01 -7.84761369e-01 -8.37429523e-01 -9.24716890e-01
  -2.16755629e-01]
 [ 2.61724815e-02 -2.82746321e-03 -1.48569187e-02 -1.51812080e-02
   7.44495213e-01]
 [-2.32015867e-02 -2.71017943e-02 -9.38411336e-03 -7.39775086e-03
  -4.37539905e-01]
 [ 5.94163984e-02  1.75790623e-01  2.00553626e-01 -1.13552608e-01
   1.48058638e-01]
 [-1.98653415e-01  9.17177871e-02  5.78298345e-02 -1.05380856e-01
  -4.08814102e-02]
 [ 5.91143779e-03  9.20170173e-03 -3.34189832e-02 -4.93440591e-02
  -4.39488649e-01]
 [-5.68043664e-02  5.59088700e-02 -5.68129355e-03  2.43952088e-02
   5.43161333e-01]
 [ 3.52141112e-02 -2.45916452e-02  2.45184880e-02  4.78404947e-03
   1.70202851e+00]
 [ 3.67120374e-03  3.90253868e-03  4.82603395e-03 -1.86242431e-03
   5.03396034e-01]
 [-4.92724292e-02 -2.41411403e-01 -7.42048249e-02  1.45813704e-01
   6.95984602e-01]
 [-2.26312838e-02 -1.12876380e-02 -1.75849088e-02 -1.73165444e-02
   3.54115278e-01]
 [ 1.43038571e-01 -1.89111363e-02  1.17659539e-01 -9.38276276e-02
   4.79829833e-02]
 [ 2.45378949e-02 -1.61405858e-02 -9.37791821e-03 -2.06089462e-03
   4.45666552e-01]
 [ 4.90463059e-03  3.09183765e-02  3.97181734e-02 -1.87212434e-02
  -5.81464708e-01]
 [-2.28448194e-02 -2.32254174e-02 -5.49036497e-03  6.56349957e-03
  -3.33657473e-01]
 [-2.65996102e-02 -1.87421236e-02 -4.56076907e-03  2.43732259e-02
  -1.31189203e+00]
 [-1.61757335e-01 -1.10860668e-01 -1.22501865e-01 -2.25760221e-01
  -6.95561394e-02]
 [ 3.51461349e-03 -4.97732460e-02  1.91589836e-02  6.07078820e-02
   1.91401219e+00]
 [-2.25853935e-01  7.88305048e-03 -2.28978135e-02 -2.30104268e-01
  -1.58818513e-01]
 [ 1.48835659e-01 -1.65559083e-01 -1.71363503e-01  2.04437017e-01
   1.99291036e-01]
 [-3.53766978e-02  4.05849069e-02  2.22982913e-02  1.93209834e-02
  -1.31091595e+00]
 [-6.16481416e-02  8.60585272e-02  8.72934237e-02  1.96763799e-01
   2.05595329e-01]
 [-8.19007158e-02 -1.60992965e-01 -1.73830286e-01  2.76336391e-02
  -1.71854272e-02]
 [-1.99638004e-03 -1.91598607e-03  1.91389006e-02  1.99125130e-02
  -9.05138612e-01]
 [ 2.50767078e-02  1.93377405e-01  5.00803143e-02 -1.01252161e-01
   1.30408990e+00]
 [-1.47555312e-02 -4.43227356e-03 -3.82985570e-03 -8.97485018e-03
  -7.74151742e-01]
 [ 3.78487229e-01  4.22741592e-01  3.85106802e-01  3.80600512e-01
   5.32994270e-02]
 [-2.12123208e-02  4.18190584e-02 -1.18197864e-02  4.18146439e-02
  -2.19162896e-01]
 [ 6.05584346e-02  4.09463188e-03 -7.36083984e-02 -6.99889362e-02
  -2.48183623e-01]
 [ 1.99286427e-04 -2.37807259e-02  1.23887486e-03 -2.80508213e-02
   3.82343858e-01]
 [ 2.20502215e-03 -9.32552665e-03 -5.64964395e-03  4.35690694e-02
  -4.69014615e-01]
 [ 2.08790302e-01  2.58232355e-01  2.41811335e-01  2.57518232e-01
   3.31581049e-02]
 [ 1.37207061e-02  3.59803960e-02  8.96790717e-03 -1.51660349e-02
   5.46821594e-01]
 [-1.04439603e-02  3.55340913e-02 -5.53294783e-03 -2.41312641e-03
  -4.49020326e-01]
 [ 3.44246849e-02 -7.26886373e-03 -1.01997247e-02 -8.09472927e-04
  -8.75983059e-01]
 [ 2.66855489e-03 -5.50844008e-04 -9.60494950e-03 -1.24006579e-02
   2.75123507e-01]
 [-3.39658320e-04 -1.52036047e-03  2.26515140e-02 -3.56490142e-03
   3.26378405e-01]
 [-3.51227224e-02 -3.69683765e-02 -4.39248793e-02 -5.75025976e-02
   2.08101422e-01]
 [-2.75565796e-02  1.64754838e-02  6.88600354e-03 -8.14276189e-03
  -8.34523559e-01]
 [-2.04929542e-02  2.27008876e-03  6.31368905e-03  3.59379873e-03
   3.50828499e-01]
 [ 3.52948233e-02  1.05752787e-02  5.41211143e-02 -1.63813848e-02
   4.31482136e-01]
 [-8.37164968e-02  9.77580994e-02 -1.16266720e-01 -4.47892733e-02
   1.22700311e-01]
 [ 6.15228079e-02 -1.31176829e-01  5.03597595e-02  1.54567748e-01
  -5.93171269e-02]
 [ 3.78380641e-02  2.11276174e-01 -1.85472757e-01  1.25119209e-01
  -1.58702135e-01]
 [ 1.21040307e-02 -1.65423173e-02 -1.71481539e-02  5.02756536e-02
   6.54219806e-01]
 [ 2.76897520e-01  1.02786727e-01 -1.03098467e-01 -7.10705668e-02
  -2.43298765e-02]
 [ 2.68487036e-01  2.53706217e-01  2.59579927e-01  2.65086889e-01
   1.04470007e-01]
 [-1.63062960e-01 -5.50793521e-02  1.07342772e-01 -4.68496094e-03
  -5.42276986e-02]
 [ 1.84501857e-02  7.85614550e-03 -1.40549466e-02 -4.30005416e-03
  -3.68869394e-01]
 [ 7.63795227e-02  1.37739172e-02  6.12942129e-02 -1.02019152e-02
   1.09378469e+00]
 [ 9.84142572e-02 -6.46779463e-02 -1.82274669e-01 -1.15012020e-01
  -1.74923196e-01]
 [-2.74684839e-02 -4.12807688e-02 -1.10183164e-01 -2.18324270e-02
  -3.28417599e-01]
 [ 9.09987185e-03  1.01079345e-02  7.08337408e-03  1.06236211e-03
  -9.96553957e-01]
 [-1.17234901e-01  3.35476138e-02 -2.06356049e-01 -1.32136554e-01
  -5.95614091e-02]
 [ 1.51797399e-01 -1.51536530e-02 -4.09349576e-02  9.61674303e-02
   3.69090065e-02]
 [-2.36623045e-02 -8.95589143e-02  4.51581180e-02  1.14687487e-01
   1.61705747e-01]
 [ 2.56277025e-02  1.19349267e-03  3.18282004e-03 -9.80233657e-04
  -1.32278728e+00]
 [ 1.05930464e-02 -5.91804320e-03  7.16020772e-03  2.88691688e-02
  -4.74661589e-01]
 [-3.17361462e-03 -2.19960567e-02 -1.06991660e-02  4.82265577e-02
   1.96088448e-01]
 [ 4.80265021e-02 -2.41996441e-02  6.08884469e-02 -6.51632845e-02
   2.24290013e-01]
 [ 3.90822053e-01  3.04356277e-01  3.12703729e-01  3.39057386e-01
   1.01665109e-01]
 [ 1.77619547e-01  1.31699502e-01  1.77490339e-01  1.56231537e-01
  -5.00425603e-03]
 [ 2.45041952e-01  2.66661853e-01  2.96304941e-01  2.21567884e-01
   7.10815340e-02]
 [ 2.79993534e-01 -2.35655338e-01 -2.84601957e-01 -1.62724614e-01
   9.22149629e-05]
 [-2.34787818e-03  1.54984146e-02 -5.71450219e-03  4.63696755e-03
   1.14259934e+00]
 [ 1.56022226e-02  1.10539822e-02  1.43716787e-03 -4.40574847e-02
   1.32537854e+00]
 [ 1.22063860e-01  7.65153766e-02  1.01252288e-01  2.88485549e-02
   1.29002139e-01]
 [-1.14975661e-01 -4.63769436e-02  9.82176587e-02  1.53372943e-01
   4.09560874e-02]
 [ 1.16924897e-01  1.10262863e-01  1.17826328e-01  1.05036065e-01
   2.77147554e-02]
 [-2.52877735e-02 -2.28196066e-02  1.28689051e-01 -9.02266651e-02
  -2.86417571e-03]
 [ 1.56115875e-01 -1.02073856e-01  4.30111438e-02  1.96280092e-01
   9.71645059e-04]
 [ 3.03072222e-02  6.08625216e-03 -3.48981842e-02 -1.37558766e-02
   1.21400094e+00]
 [ 1.77612640e-02  2.67985233e-05  7.26146298e-03  1.14647085e-02
   7.27177978e-01]
 [ 1.08081140e-01 -1.44741461e-01 -1.17916904e-01  3.71080004e-02
  -2.53860410e-02]
 [ 6.55086711e-02  1.71071097e-01 -2.57045895e-01 -1.45119742e-01
   1.85263101e-02]
 [-3.24131511e-02  1.59051479e-03 -2.08623707e-02 -1.01423329e-02
  -1.11899996e+00]
 [-3.99968699e-02 -6.94076903e-03 -4.07020524e-02  2.93556247e-02
   6.74492359e-01]
 [ 1.73861813e-02  2.09682360e-02 -1.62485272e-01  4.89797480e-02
  -1.88054070e-01]
 [-1.95212975e-01 -6.63565472e-02 -1.98003545e-01 -5.05446754e-02
  -8.38662386e-02]
 [ 2.05657817e-02  1.02904523e-02  2.34703030e-02 -5.05378982e-03
  -1.00661874e+00]
 [-1.31121412e-01 -1.41662294e-02 -1.73403993e-01  8.78460035e-02
   2.68880516e-01]]: "
2018-04-24 14:12:14,564 (dqn_agent.py:162) DEBUG: "var_name: target_q/fully_connected_1/biases:0, var_value [ 0.3226075   0.4083015   0.31867546  0.32637635 -0.10090829]: "
2018-04-24 14:12:14,624 (dqn_main.py:207) DEBUG: "saving graph 0 -> steps: 
[], 
step_length: 0, rl_cost: 0, reward: -1.0, str_out=max_reward=-inf < ep_reward=-1.0"
2018-04-24 14:12:14,739 (dqn_main.py:207) DEBUG: "saving graph 42 -> steps: 
[(3, 3), (4, 2), (2, 2), (2, 3)], 
step_length: 4, rl_cost: 9.0, reward: -1.1102230246251565e-16, str_out=max_reward=-1.0 < ep_reward=-1.1102230246251565e-16"
2018-04-24 14:12:15,083 (dqn_main.py:207) DEBUG: "saving graph 613 -> steps: 
[(1, 4), (1, 5), (2, 6), (3, 6), (6, 8), (5, 7)], 
step_length: 6, rl_cost: 43.0, reward: 1.1999999999999988, str_out=max_reward=-1.1102230246251565e-16 < ep_reward=1.1999999999999988"
2018-04-24 14:12:15,257 (dqn_main.py:207) DEBUG: "saving graph 1354 -> steps: 
[(1, 2), (4, 3), (4, 4)], 
step_length: 3, rl_cost: 13.0, reward: 1.2000000000000002, str_out=max_reward=1.1999999999999988 < ep_reward=1.2000000000000002"
2018-04-24 14:12:15,974 (dqn_main.py:207) DEBUG: "saving graph 4603 -> steps: 
[(1, 3), (1, 2), (2, 2)], 
step_length: 3, rl_cost: 3.0, reward: 1.2999999999999998, str_out=max_reward=1.2000000000000002 < ep_reward=1.2999999999999998"
2018-04-24 14:12:16,243 (dqn_main.py:207) DEBUG: "saving graph 6200 -> steps: 
[(1, 2), (1, 3), (2, 5), (3, 4), (3, 5)], 
step_length: 5, rl_cost: 17.0, reward: 1.5999999999999996, str_out=max_reward=1.2999999999999998 < ep_reward=1.5999999999999996"
2018-04-24 14:12:18,174 (dqn_main.py:207) DEBUG: "saving graph 14090 -> steps: 
[(1, 2), (2, 2), (2, 3), (3, 3), (3, 4)], 
step_length: 5, rl_cost: 11.0, reward: 2.5999999999999996, str_out=max_reward=1.5999999999999996 < ep_reward=2.5999999999999996"
2018-04-24 14:12:43,235 (dqn_main.py:244) DEBUG: "Episode 10000, mean reward over last 10000 episodes: -1.3178"
2018-04-24 14:12:43,235 (dqn_main.py:245) DEBUG: "Epsilon: 0.994379500000185"
2018-04-24 14:12:43,235 (dqn_main.py:246) DEBUG: "RL steps: [], reward: -1.2, done: True"
2018-04-24 14:12:43,235 (dqn_main.py:247) DEBUG: "Steps: 0, coords: 48"
2018-04-24 14:13:17,403 (dqn_main.py:207) DEBUG: "saving graph 77232 -> steps: 
[(1, 2), (2, 2), (3, 3), (2, 5), (3, 4)], 
step_length: 5, rl_cost: 13.0, reward: 2.7999999999999994, str_out=max_reward=2.5999999999999996 < ep_reward=2.7999999999999994"
2018-04-24 14:13:22,760 (dqn_main.py:207) DEBUG: "saving graph 80678 -> steps: 
[(1, 2), (2, 3), (3, 4), (4, 4), (4, 5), (3, 3)], 
step_length: 6, rl_cost: 22.0, reward: 3.499999999999999, str_out=max_reward=2.7999999999999994 < ep_reward=3.499999999999999"
2018-04-24 14:14:28,049 (dqn_main.py:244) DEBUG: "Episode 20000, mean reward over last 10000 episodes: -1.3107000000000002"
2018-04-24 14:14:28,049 (dqn_main.py:245) DEBUG: "Epsilon: 0.941774500001917"
2018-04-24 14:14:28,049 (dqn_main.py:246) DEBUG: "RL steps: [], reward: -1.4, done: True"
2018-04-24 14:14:28,050 (dqn_main.py:247) DEBUG: "Steps: 0, coords: 48"
2018-04-24 14:14:57,279 (dqn_main.py:207) DEBUG: "saving graph 127498 -> steps: 
[(2, 2), (2, 3), (3, 4), (2, 4), (2, 6), (4, 5), (6, 3), (7, 3), (7, 4)], 
step_length: 9, rl_cost: 51.0, reward: 4.700000000000002, str_out=max_reward=3.499999999999999 < ep_reward=4.700000000000002"
2018-04-24 14:17:20,360 (dqn_main.py:244) DEBUG: "Episode 30000, mean reward over last 10000 episodes: -1.259"
2018-04-24 14:17:20,360 (dqn_main.py:245) DEBUG: "Epsilon: 0.8825365000038673"
2018-04-24 14:17:20,360 (dqn_main.py:246) DEBUG: "RL steps: [], reward: -2.0999999999999996, done: True"
2018-04-24 14:17:20,360 (dqn_main.py:247) DEBUG: "Steps: 0, coords: 48"
2018-04-24 14:20:38,460 (dqn_main.py:207) DEBUG: "saving graph 249057 -> steps: 
[(1, 2), (2, 3), (2, 2), (4, 3), (3, 5), (4, 4), (4, 2), (3, 2), (2, 4), (3, 4), (4, 5), (5, 5), (3, 6)], 
step_length: 13, rl_cost: 47.0, reward: 5.500000000000004, str_out=max_reward=4.700000000000002 < ep_reward=5.500000000000004"
2018-04-24 14:21:07,053 (dqn_main.py:244) DEBUG: "Episode 40000, mean reward over last 10000 episodes: -1.19268"
2018-04-24 14:21:07,054 (dqn_main.py:245) DEBUG: "Epsilon: 0.8121799000061837"
2018-04-24 14:21:07,054 (dqn_main.py:246) DEBUG: "RL steps: [], reward: -1.4, done: True"
2018-04-24 14:21:07,054 (dqn_main.py:247) DEBUG: "Steps: 0, coords: 48"
2018-04-24 14:24:01,610 (dqn_main.py:207) DEBUG: "saving graph 319450 -> steps: 
[(2, 2), (2, 3), (2, 4), (3, 5), (3, 6), (2, 6), (1, 6), (1, 5), (1, 4), (3, 4), (4, 5), (5, 5), (5, 7)], 
step_length: 13, rl_cost: 55.0, reward: 5.9000000000000155, str_out=max_reward=5.500000000000004 < ep_reward=5.9000000000000155"
2018-04-24 14:24:44,884 (dqn_main.py:207) DEBUG: "saving graph 335984 -> steps: 
[(3, 2), (4, 4), (3, 4), (5, 5), (4, 5), (3, 5), (2, 5), (2, 7), (3, 6), (6, 6), (5, 7), (6, 8), (7, 6), (7, 5), (8, 6), (7, 8)], 
step_length: 16, rl_cost: 126.0, reward: 6.2000000000000135, str_out=max_reward=5.9000000000000155 < ep_reward=6.2000000000000135"
2018-04-24 14:24:58,385 (dqn_main.py:207) DEBUG: "saving graph 340371 -> steps: 
[(1, 3), (2, 3), (3, 3), (3, 4), (3, 5), (3, 6), (4, 6), (4, 7), (7, 7), (6, 7)], 
step_length: 10, rl_cost: 65.0, reward: 7.100000000000003, str_out=max_reward=6.2000000000000135 < ep_reward=7.100000000000003"
2018-04-24 14:25:32,193 (dqn_main.py:244) DEBUG: "Episode 50000, mean reward over last 10000 episodes: -0.99911"
2018-04-24 14:25:32,193 (dqn_main.py:245) DEBUG: "Epsilon: 0.7276537000089666"
2018-04-24 14:25:32,193 (dqn_main.py:246) DEBUG: "RL steps: [], reward: -1.1, done: True"
2018-04-24 14:25:32,193 (dqn_main.py:247) DEBUG: "Steps: 0, coords: 48"
2018-04-24 14:31:41,326 (dqn_main.py:244) DEBUG: "Episode 60000, mean reward over last 10000 episodes: -0.71064"
2018-04-24 14:31:41,326 (dqn_main.py:245) DEBUG: "Epsilon: 0.6248467000123513"
2018-04-24 14:31:41,327 (dqn_main.py:246) DEBUG: "RL steps: [], reward: -1.0, done: True"
2018-04-24 14:31:41,327 (dqn_main.py:247) DEBUG: "Steps: 0, coords: 48"
2018-04-24 14:32:46,466 (dqn_main.py:207) DEBUG: "saving graph 486503 -> steps: 
[(1, 2), (2, 3), (2, 2), (2, 4), (1, 5), (3, 6), (4, 5), (3, 4), (3, 5), (4, 6), (5, 5), (5, 6), (2, 5), (5, 8), (6, 8), (6, 6), (8, 6)], 
step_length: 17, rl_cost: 101.0, reward: 7.3000000000000185, str_out=max_reward=7.100000000000003 < ep_reward=7.3000000000000185"
2018-04-24 14:32:52,857 (dqn_main.py:207) DEBUG: "saving graph 488858 -> steps: 
[(1, 2), (2, 2), (2, 3), (1, 4), (2, 4), (4, 5), (3, 6), (2, 5), (2, 6), (3, 5), (4, 6), (5, 7), (5, 6), (5, 5)], 
step_length: 14, rl_cost: 64.0, reward: 8.400000000000006, str_out=max_reward=7.3000000000000185 < ep_reward=8.400000000000006"
2018-04-24 14:38:23,625 (dqn_main.py:207) DEBUG: "saving graph 596829 -> steps: 
[(2, 2), (2, 3), (2, 4), (4, 5), (4, 4), (3, 4), (3, 5), (2, 5), (3, 6), (4, 6), (5, 6), (8, 5), (7, 5), (8, 6), (6, 7), (7, 7), (8, 8)], 
step_length: 17, rl_cost: 126.0, reward: 9.900000000000011, str_out=max_reward=8.400000000000006 < ep_reward=9.900000000000011"
2018-04-24 14:39:42,547 (dqn_main.py:244) DEBUG: "Episode 70000, mean reward over last 10000 episodes: -0.19287"
2018-04-24 14:39:42,547 (dqn_main.py:245) DEBUG: "Epsilon: 0.4843108000160105"
2018-04-24 14:39:42,547 (dqn_main.py:246) DEBUG: "RL steps: [], reward: -1.2, done: True"
2018-04-24 14:39:42,547 (dqn_main.py:247) DEBUG: "Steps: 0, coords: 48"
2018-04-24 14:48:25,477 (dqn_main.py:207) DEBUG: "saving graph 809737 -> steps: 
[(2, 2), (2, 3), (4, 3), (3, 3), (3, 5), (4, 5), (2, 5), (2, 4), (1, 4), (4, 4), (5, 3), (8, 4), (8, 5), (7, 5), (5, 6), (5, 5), (5, 2), (3, 2)], 
step_length: 18, rl_cost: 98.0, reward: 11.200000000000015, str_out=max_reward=9.900000000000011 < ep_reward=11.200000000000015"
2018-04-24 14:50:27,114 (dqn_main.py:244) DEBUG: "Episode 80000, mean reward over last 10000 episodes: 0.5005900000000001"
2018-04-24 14:50:27,115 (dqn_main.py:245) DEBUG: "Epsilon: 0.2688436000098146"
2018-04-24 14:50:27,115 (dqn_main.py:246) DEBUG: "RL steps: [], reward: -1.2, done: True"
2018-04-24 14:50:27,115 (dqn_main.py:247) DEBUG: "Steps: 0, coords: 48"
2018-04-24 14:51:26,513 (dqn_main.py:207) DEBUG: "saving graph 888912 -> steps: 
[(1, 2), (2, 2), (2, 3), (4, 5), (3, 5), (1, 5), (1, 6), (2, 7), (2, 6), (3, 6), (4, 6), (5, 6), (5, 5), (6, 6), (7, 6), (8, 5), (8, 4), (8, 6)], 
step_length: 18, rl_cost: 108.0, reward: 12.80000000000001, str_out=max_reward=11.200000000000015 < ep_reward=12.80000000000001"
2018-04-24 15:13:58,794 (dqn_main.py:244) DEBUG: "Episode 90000, mean reward over last 10000 episodes: -1.9575299999999989"
2018-04-24 15:13:58,795 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 15:13:58,795 (dqn_main.py:246) DEBUG: "RL steps: [(2, 2), (4, 5), (3, 4), (5, 6), (6, 6)], reward: -4.0000000000000036, done: True"
2018-04-24 15:13:58,795 (dqn_main.py:247) DEBUG: "Steps: 5, coords: 48"
2018-04-24 15:41:52,267 (dqn_main.py:244) DEBUG: "Episode 100000, mean reward over last 10000 episodes: -3.812509999999997"
2018-04-24 15:41:52,267 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 15:41:52,267 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (2, 2), (2, 3)], reward: -0.8999999999999999, done: True"
2018-04-24 15:41:52,267 (dqn_main.py:247) DEBUG: "Steps: 3, coords: 48"
2018-04-24 16:04:34,525 (dqn_main.py:207) DEBUG: "saving graph 3214836 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (2, 5), (3, 5), (4, 5), (1, 5), (2, 6), (3, 6), (4, 4), (4, 7), (5, 7), (6, 7), (7, 6), (7, 5), (7, 4), (8, 4), (8, 5)], 
step_length: 20, rl_cost: 113.0, reward: 12.90000000000002, str_out=max_reward=12.80000000000001 < ep_reward=12.90000000000002"
2018-04-24 16:06:34,460 (dqn_main.py:207) DEBUG: "saving graph 3273778 -> steps: 
[(1, 2), (2, 2), (3, 2), (2, 3), (1, 3), (1, 4), (2, 4), (2, 6), (2, 5), (3, 4), (3, 5), (4, 4), (4, 5), (4, 6), (6, 6), (7, 6), (8, 6), (6, 7), (5, 6), (4, 7)], 
step_length: 20, rl_cost: 109.0, reward: 14.400000000000013, str_out=max_reward=12.90000000000002 < ep_reward=14.400000000000013"
2018-04-24 16:09:17,589 (dqn_main.py:244) DEBUG: "Episode 110000, mean reward over last 10000 episodes: -1.8262999999999963"
2018-04-24 16:09:17,589 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 16:09:17,589 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (3, 4), (4, 2), (5, 3), (2, 2), (2, 4), (1, 4)], reward: -2.1000000000000036, done: True"
2018-04-24 16:09:17,589 (dqn_main.py:247) DEBUG: "Steps: 7, coords: 48"
2018-04-24 16:17:46,570 (dqn_main.py:207) DEBUG: "saving graph 3613230 -> steps: 
[(1, 2), (2, 2), (3, 2), (4, 2), (3, 3), (3, 4), (2, 4), (2, 5), (1, 5), (1, 4), (2, 6), (3, 5), (4, 4), (4, 5), (4, 6), (4, 7), (5, 7), (5, 6), (5, 5), (6, 6), (6, 8), (6, 7), (8, 6), (7, 6), (7, 5), (7, 4)], 
step_length: 26, rl_cost: 147.0, reward: 16.999999999999982, str_out=max_reward=14.400000000000013 < ep_reward=16.999999999999982"
2018-04-24 16:27:33,180 (dqn_main.py:207) DEBUG: "saving graph 3900292 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (2, 5), (1, 5), (2, 6), (3, 6), (4, 5), (3, 4), (3, 5), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (5, 5), (5, 6), (6, 6), (6, 8), (7, 7), (7, 6), (8, 6)], 
step_length: 24, rl_cost: 117.0, reward: 18.9, str_out=max_reward=16.999999999999982 < ep_reward=18.9"
2018-04-24 16:33:33,658 (dqn_main.py:207) DEBUG: "saving graph 4070771 -> steps: 
[(1, 2), (2, 2), (3, 3), (2, 3), (2, 4), (2, 5), (1, 5), (1, 4), (3, 5), (3, 4), (5, 5), (5, 3), (5, 2), (4, 2), (4, 3), (4, 4), (5, 6), (6, 6), (7, 6), (7, 7), (8, 5), (7, 5), (8, 8), (8, 7), (8, 6)], 
step_length: 25, rl_cost: 151.0, reward: 19.299999999999994, str_out=max_reward=18.9 < ep_reward=19.299999999999994"
2018-04-24 16:36:05,418 (dqn_main.py:244) DEBUG: "Episode 120000, mean reward over last 10000 episodes: 1.2581800000000067"
2018-04-24 16:36:05,418 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 16:36:05,418 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (3, 2), (2, 2), (1, 4), (1, 3), (2, 4), (2, 5), (2, 6), (4, 5)], reward: 4.3000000000000025, done: True"
2018-04-24 16:36:05,418 (dqn_main.py:247) DEBUG: "Steps: 9, coords: 48"
2018-04-24 16:40:37,052 (dqn_main.py:207) DEBUG: "saving graph 4271217 -> steps: 
[(1, 2), (2, 2), (2, 3), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (3, 5), (2, 5), (1, 5), (2, 6), (2, 7), (1, 6), (3, 6), (3, 4), (2, 4), (1, 4), (5, 3), (5, 2), (5, 5), (5, 6), (6, 6), (7, 6), (7, 5), (8, 5), (8, 4), (7, 4), (4, 6), (4, 5)], 
step_length: 30, rl_cost: 123.0, reward: 20.499999999999957, str_out=max_reward=19.299999999999994 < ep_reward=20.499999999999957"
2018-04-24 16:48:49,633 (dqn_main.py:207) DEBUG: "saving graph 4500946 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (1, 5), (1, 4), (3, 3), (4, 2), (3, 2), (1, 6), (3, 6), (4, 6), (5, 7), (5, 6), (5, 5), (5, 3), (5, 2), (4, 3), (4, 4), (4, 5), (6, 6), (7, 6), (8, 6), (6, 7), (5, 8), (6, 8)], 
step_length: 28, rl_cost: 124.0, reward: 20.599999999999994, str_out=max_reward=20.499999999999957 < ep_reward=20.599999999999994"
2018-04-24 16:49:58,957 (dqn_main.py:207) DEBUG: "saving graph 4534308 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (2, 5), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 2), (4, 2), (5, 3), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (6, 7)], 
step_length: 26, rl_cost: 135.0, reward: 21.099999999999987, str_out=max_reward=20.599999999999994 < ep_reward=21.099999999999987"
2018-04-24 16:50:06,047 (dqn_main.py:207) DEBUG: "saving graph 4537606 -> steps: 
[(1, 2), (2, 2), (2, 3), (1, 4), (1, 5), (1, 6), (2, 6), (2, 7), (3, 6), (3, 5), (3, 4), (2, 4), (2, 5), (3, 3), (3, 2), (4, 2), (5, 3), (5, 2), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 5), (7, 5), (7, 4), (8, 4), (4, 3)], 
step_length: 29, rl_cost: 114.0, reward: 21.19999999999996, str_out=max_reward=21.099999999999987 < ep_reward=21.19999999999996"
2018-04-24 16:51:42,382 (dqn_main.py:207) DEBUG: "saving graph 4581718 -> steps: 
[(1, 2), (2, 2), (2, 3), (1, 3), (1, 4), (2, 6), (2, 5), (1, 5), (3, 3), (3, 2), (3, 4), (5, 5), (5, 6), (4, 6), (5, 3), (5, 2), (4, 2), (4, 3), (4, 4), (4, 5), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4)], 
step_length: 27, rl_cost: 128.0, reward: 21.599999999999998, str_out=max_reward=21.19999999999996 < ep_reward=21.599999999999998"
2018-04-24 16:56:28,900 (dqn_main.py:207) DEBUG: "saving graph 4712107 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (4, 5), (5, 5), (4, 3), (4, 2), (5, 3), (4, 4), (3, 4), (3, 3), (3, 2), (5, 6), (4, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 3), (8, 4)], 
step_length: 29, rl_cost: 126.0, reward: 22.399999999999995, str_out=max_reward=21.599999999999998 < ep_reward=22.399999999999995"
2018-04-24 17:01:53,295 (dqn_main.py:244) DEBUG: "Episode 130000, mean reward over last 10000 episodes: 5.079740000000009"
2018-04-24 17:01:53,295 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 17:01:53,295 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (2, 6), (2, 5), (3, 3), (3, 2), (5, 2), (5, 5), (5, 6), (5, 7), (4, 6), (4, 5), (5, 3), (4, 2), (4, 3), (4, 4)], reward: 12.900000000000018, done: True"
2018-04-24 17:01:53,295 (dqn_main.py:247) DEBUG: "Steps: 20, coords: 48"
2018-04-24 17:04:01,034 (dqn_main.py:207) DEBUG: "saving graph 4917157 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 3), (3, 2), (3, 4), (3, 5), (2, 5), (4, 4), (4, 6), (5, 6), (6, 6), (6, 7), (7, 6), (7, 5), (8, 5), (8, 4), (7, 4), (5, 5), (5, 3), (5, 2), (4, 2), (4, 3), (4, 5)], 
step_length: 30, rl_cost: 139.0, reward: 22.79999999999997, str_out=max_reward=22.399999999999995 < ep_reward=22.79999999999997"
2018-04-24 17:06:53,420 (dqn_main.py:207) DEBUG: "saving graph 4992988 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (2, 5), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (8, 4), (7, 5), (7, 4), (4, 6), (4, 7), (6, 7), (5, 7)], 
step_length: 30, rl_cost: 139.0, reward: 22.799999999999983, str_out=max_reward=22.79999999999997 < ep_reward=22.799999999999983"
2018-04-24 17:07:00,020 (dqn_main.py:207) DEBUG: "saving graph 4996531 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (2, 5), (1, 3), (3, 2), (4, 2), (4, 4), (3, 5), (3, 6), (4, 6), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (8, 4), (5, 3), (5, 2), (4, 3), (4, 5), (5, 5), (4, 7), (5, 7), (2, 7), (1, 7)], 
step_length: 31, rl_cost: 128.0, reward: 24.599999999999973, str_out=max_reward=22.799999999999983 < ep_reward=24.599999999999973"
2018-04-24 17:13:52,054 (dqn_main.py:207) DEBUG: "saving graph 5175036 -> steps: 
[(1, 2), (2, 2), (2, 3), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (8, 7), (7, 7), (4, 6), (4, 7), (5, 7), (5, 6), (2, 7), (1, 7)], 
step_length: 32, rl_cost: 161.0, reward: 24.99999999999995, str_out=max_reward=24.599999999999973 < ep_reward=24.99999999999995"
2018-04-24 17:16:05,551 (dqn_main.py:207) DEBUG: "saving graph 5232892 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (2, 5), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 3), (3, 2), (4, 2), (4, 3), (5, 3), (5, 2), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (8, 7), (6, 7), (4, 7), (5, 7)], 
step_length: 32, rl_cost: 157.0, reward: 26.399999999999984, str_out=max_reward=24.99999999999995 < ep_reward=26.399999999999984"
2018-04-24 17:21:21,999 (dqn_main.py:207) DEBUG: "saving graph 5363280 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 3), (3, 4), (2, 5), (1, 5), (2, 6), (3, 6), (3, 5), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (4, 6), (4, 7), (5, 7), (5, 8), (6, 8), (6, 7)], 
step_length: 33, rl_cost: 136.0, reward: 26.59999999999997, str_out=max_reward=26.399999999999984 < ep_reward=26.59999999999997"
2018-04-24 17:26:52,196 (dqn_main.py:244) DEBUG: "Episode 140000, mean reward over last 10000 episodes: 9.633729999999993"
2018-04-24 17:26:52,196 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 17:26:52,196 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (2, 2), (3, 4), (2, 3), (2, 4), (3, 2), (3, 3)], reward: -0.2000000000000015, done: True"
2018-04-24 17:26:52,196 (dqn_main.py:247) DEBUG: "Steps: 7, coords: 48"
2018-04-24 17:31:39,270 (dqn_main.py:207) DEBUG: "saving graph 5630595 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 32, rl_cost: 148.0, reward: 26.899999999999977, str_out=max_reward=26.59999999999997 < ep_reward=26.899999999999977"
2018-04-24 17:35:19,327 (dqn_main.py:207) DEBUG: "saving graph 5724696 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (8, 4), (7, 5), (7, 4), (5, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 33, rl_cost: 146.0, reward: 27.099999999999973, str_out=max_reward=26.899999999999977 < ep_reward=27.099999999999973"
2018-04-24 17:37:31,831 (dqn_main.py:207) DEBUG: "saving graph 5780969 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 3), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (5, 7), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (8, 7), (5, 5), (5, 3), (5, 2)], 
step_length: 33, rl_cost: 147.0, reward: 27.199999999999974, str_out=max_reward=27.099999999999973 < ep_reward=27.199999999999974"
2018-04-24 17:39:36,755 (dqn_main.py:207) DEBUG: "saving graph 5828735 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (2, 5), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (5, 7), (5, 6), (5, 5), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 7), (6, 8), (5, 8)], 
step_length: 34, rl_cost: 143.0, reward: 27.499999999999957, str_out=max_reward=27.199999999999974 < ep_reward=27.499999999999957"
2018-04-24 17:41:54,218 (dqn_main.py:207) DEBUG: "saving graph 5880913 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (8, 4), (7, 5), (7, 4), (5, 3), (5, 2), (4, 6), (5, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 34, rl_cost: 140.0, reward: 27.899999999999963, str_out=max_reward=27.499999999999957 < ep_reward=27.899999999999963"
2018-04-24 17:42:17,310 (dqn_main.py:207) DEBUG: "saving graph 5889969 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 34, rl_cost: 145.0, reward: 28.099999999999966, str_out=max_reward=27.899999999999963 < ep_reward=28.099999999999966"
2018-04-24 17:43:15,693 (dqn_main.py:207) DEBUG: "saving graph 5913295 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (1, 3), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 35, rl_cost: 136.0, reward: 28.499999999999968, str_out=max_reward=28.099999999999966 < ep_reward=28.499999999999968"
2018-04-24 17:45:02,759 (dqn_main.py:207) DEBUG: "saving graph 5959149 -> steps: 
[(1, 3), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (8, 4), (7, 5), (7, 4), (5, 3), (5, 2), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 35, rl_cost: 139.0, reward: 29.09999999999997, str_out=max_reward=28.499999999999968 < ep_reward=29.09999999999997"
2018-04-24 17:50:16,727 (dqn_main.py:207) DEBUG: "saving graph 6083548 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (2, 5), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (5, 3), (5, 2), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 35, rl_cost: 132.0, reward: 29.099999999999973, str_out=max_reward=29.09999999999997 < ep_reward=29.099999999999973"
2018-04-24 17:52:38,278 (dqn_main.py:244) DEBUG: "Episode 150000, mean reward over last 10000 episodes: 11.980939999999988"
2018-04-24 17:52:38,279 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 17:52:38,279 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (2, 5), (3, 5), (3, 4), (3, 3)], reward: 8.600000000000001, done: True"
2018-04-24 17:52:38,279 (dqn_main.py:247) DEBUG: "Steps: 12, coords: 48"
2018-04-24 17:54:18,761 (dqn_main.py:207) DEBUG: "saving graph 6183778 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (5, 7), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 2), (6, 2), (6, 3), (5, 3), (5, 5), (8, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 38, rl_cost: 152.0, reward: 30.499999999999957, str_out=max_reward=29.099999999999973 < ep_reward=30.499999999999957"
2018-04-24 18:01:15,244 (dqn_main.py:207) DEBUG: "saving graph 6355910 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 3), (1, 4), (1, 5), (1, 6), (2, 6), (2, 5), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 7), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 37, rl_cost: 157.0, reward: 30.599999999999966, str_out=max_reward=30.499999999999957 < ep_reward=30.599999999999966"
2018-04-24 18:09:34,914 (dqn_main.py:207) DEBUG: "saving graph 6557892 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 3), (1, 5), (1, 6), (2, 6), (2, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (3, 5), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (4, 6), (3, 6), (4, 7), (5, 7), (5, 3), (5, 2), (6, 2), (6, 3), (8, 7), (7, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 40, rl_cost: 160.0, reward: 31.29999999999994, str_out=max_reward=30.599999999999966 < ep_reward=31.29999999999994"
2018-04-24 18:18:23,408 (dqn_main.py:244) DEBUG: "Episode 160000, mean reward over last 10000 episodes: 13.670509999999982"
2018-04-24 18:18:23,412 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 18:18:23,413 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 5), (3, 4), (4, 2), (5, 2)], reward: 3.5000000000000338, done: True"
2018-04-24 18:18:23,413 (dqn_main.py:247) DEBUG: "Steps: 15, coords: 48"
2018-04-24 18:26:03,967 (dqn_main.py:207) DEBUG: "saving graph 6950019 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (2, 5), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (3, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8)], 
step_length: 38, rl_cost: 151.0, reward: 31.399999999999963, str_out=max_reward=31.29999999999994 < ep_reward=31.399999999999963"
2018-04-24 18:32:43,415 (dqn_main.py:207) DEBUG: "saving graph 7096101 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 3)], 
step_length: 39, rl_cost: 176.0, reward: 32.49999999999996, str_out=max_reward=31.399999999999963 < ep_reward=32.49999999999996"
2018-04-24 18:48:21,226 (dqn_main.py:244) DEBUG: "Episode 170000, mean reward over last 10000 episodes: 14.29212999999998"
2018-04-24 18:48:21,226 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 18:48:21,227 (dqn_main.py:246) DEBUG: "RL steps: [(1, 3)], reward: -1.5999999999999999, done: True"
2018-04-24 18:48:21,227 (dqn_main.py:247) DEBUG: "Steps: 1, coords: 48"
2018-04-24 18:49:13,801 (dqn_main.py:207) DEBUG: "saving graph 7464167 -> steps: 
[(1, 2), (1, 3), (2, 3), (2, 2), (2, 4), (1, 4), (1, 5), (1, 6), (2, 7), (2, 6), (2, 5), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (6, 7), (8, 7), (7, 7), (6, 8), (5, 8), (5, 7), (3, 6)], 
step_length: 41, rl_cost: 150.0, reward: 32.79999999999995, str_out=max_reward=32.49999999999996 < ep_reward=32.79999999999995"
2018-04-24 18:49:18,581 (dqn_main.py:207) DEBUG: "saving graph 7465632 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 3)], 
step_length: 40, rl_cost: 159.0, reward: 32.899999999999956, str_out=max_reward=32.79999999999995 < ep_reward=32.899999999999956"
2018-04-24 18:50:19,372 (dqn_main.py:207) DEBUG: "saving graph 7488365 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 3)], 
step_length: 40, rl_cost: 164.0, reward: 33.09999999999995, str_out=max_reward=32.899999999999956 < ep_reward=33.09999999999995"
2018-04-24 18:53:05,119 (dqn_main.py:207) DEBUG: "saving graph 7550457 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 5), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 5), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8)], 
step_length: 40, rl_cost: 165.0, reward: 33.19999999999996, str_out=max_reward=33.09999999999995 < ep_reward=33.19999999999996"
2018-04-24 18:54:11,728 (dqn_main.py:207) DEBUG: "saving graph 7575114 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (4, 2), (4, 3), (3, 3), (3, 2), (4, 4), (4, 5), (4, 6), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (5, 5), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8)], 
step_length: 41, rl_cost: 157.0, reward: 33.599999999999966, str_out=max_reward=33.19999999999996 < ep_reward=33.599999999999966"
2018-04-24 18:54:13,441 (dqn_main.py:207) DEBUG: "saving graph 7575500 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8)], 
step_length: 41, rl_cost: 157.0, reward: 33.799999999999955, str_out=max_reward=33.599999999999966 < ep_reward=33.799999999999955"
2018-04-24 18:56:47,379 (dqn_main.py:207) DEBUG: "saving graph 7631682 -> steps: 
[(1, 3), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (7, 5), (7, 4), (8, 4), (8, 5), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8)], 
step_length: 41, rl_cost: 156.0, reward: 33.99999999999996, str_out=max_reward=33.799999999999955 < ep_reward=33.99999999999996"
2018-04-24 18:58:50,835 (dqn_main.py:207) DEBUG: "saving graph 7678954 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 5), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (7, 8), (8, 8)], 
step_length: 41, rl_cost: 151.0, reward: 34.59999999999996, str_out=max_reward=33.99999999999996 < ep_reward=34.59999999999996"
2018-04-24 19:12:45,382 (dqn_main.py:207) DEBUG: "saving graph 7982350 -> steps: 
[(1, 3), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 7), (7, 8), (8, 8)], 
step_length: 42, rl_cost: 146.0, reward: 34.799999999999955, str_out=max_reward=34.59999999999996 < ep_reward=34.799999999999955"
2018-04-24 19:12:57,857 (dqn_main.py:207) DEBUG: "saving graph 7986182 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 5), (3, 6), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8)], 
step_length: 42, rl_cost: 153.0, reward: 34.99999999999996, str_out=max_reward=34.799999999999955 < ep_reward=34.99999999999996"
2018-04-24 19:19:04,180 (dqn_main.py:244) DEBUG: "Episode 180000, mean reward over last 10000 episodes: 15.024549999999982"
2018-04-24 19:19:04,190 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 19:19:04,190 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6)], reward: 17.6, done: True"
2018-04-24 19:19:04,191 (dqn_main.py:247) DEBUG: "Steps: 22, coords: 48"
2018-04-24 19:20:11,461 (dqn_main.py:207) DEBUG: "saving graph 8143167 -> steps: 
[(1, 3), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (3, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (7, 7), (8, 7)], 
step_length: 44, rl_cost: 165.0, reward: 35.79999999999995, str_out=max_reward=34.99999999999996 < ep_reward=35.79999999999995"
2018-04-24 19:25:21,474 (dqn_main.py:207) DEBUG: "saving graph 8259516 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 7)], 
step_length: 44, rl_cost: 164.0, reward: 36.099999999999945, str_out=max_reward=35.79999999999995 < ep_reward=36.099999999999945"
2018-04-24 19:29:21,501 (dqn_main.py:207) DEBUG: "saving graph 8344998 -> steps: 
[(1, 3), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 7)], 
step_length: 44, rl_cost: 163.0, reward: 36.49999999999995, str_out=max_reward=36.099999999999945 < ep_reward=36.49999999999995"
2018-04-24 19:31:40,700 (dqn_main.py:207) DEBUG: "saving graph 8396106 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 7)], 
step_length: 44, rl_cost: 164.0, reward: 36.599999999999945, str_out=max_reward=36.49999999999995 < ep_reward=36.599999999999945"
2018-04-24 19:33:07,987 (dqn_main.py:207) DEBUG: "saving graph 8427530 -> steps: 
[(1, 3), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 7)], 
step_length: 44, rl_cost: 163.0, reward: 37.19999999999996, str_out=max_reward=36.599999999999945 < ep_reward=37.19999999999996"
2018-04-24 19:46:48,147 (dqn_main.py:207) DEBUG: "saving graph 8734698 -> steps: 
[(1, 3), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 3), (8, 3)], 
step_length: 45, rl_cost: 161.0, reward: 37.49999999999994, str_out=max_reward=37.19999999999996 < ep_reward=37.49999999999994"
2018-04-24 19:50:37,512 (dqn_main.py:244) DEBUG: "Episode 190000, mean reward over last 10000 episodes: 16.232999999999976"
2018-04-24 19:50:37,512 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 19:50:37,512 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (8, 4), (7, 5), (7, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 7)], reward: 32.999999999999964, done: True"
2018-04-24 19:50:37,512 (dqn_main.py:247) DEBUG: "Steps: 43, coords: 48"
2018-04-24 19:54:26,841 (dqn_main.py:207) DEBUG: "saving graph 8904770 -> steps: 
[(1, 3), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 3), (8, 3)], 
step_length: 45, rl_cost: 161.0, reward: 37.69999999999995, str_out=max_reward=37.49999999999994 < ep_reward=37.69999999999995"
2018-04-24 19:54:48,012 (dqn_main.py:207) DEBUG: "saving graph 8912460 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (7, 7), (8, 7), (8, 3), (7, 3), (7, 2)], 
step_length: 47, rl_cost: 161.0, reward: 38.49999999999993, str_out=max_reward=37.69999999999995 < ep_reward=38.49999999999993"
2018-04-24 20:22:55,385 (dqn_main.py:244) DEBUG: "Episode 200000, mean reward over last 10000 episodes: 16.366869999999977"
2018-04-24 20:22:55,385 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 20:22:55,385 (dqn_main.py:246) DEBUG: "RL steps: [(1, 3), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 3), (8, 3), (8, 2)], reward: 35.89999999999995, done: True"
2018-04-24 20:22:55,385 (dqn_main.py:247) DEBUG: "Steps: 45, coords: 48"
2018-04-24 20:24:23,275 (dqn_main.py:207) DEBUG: "saving graph 9578811 -> steps: 
[(1, 2), (2, 2), (2, 3), (2, 4), (2, 5), (1, 4), (1, 3), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 3), (8, 3), (7, 2), (8, 2)], 
step_length: 47, rl_cost: 160.0, reward: 38.49999999999995, str_out=max_reward=38.49999999999993 < ep_reward=38.49999999999995"
2018-04-24 20:33:44,795 (dqn_main.py:207) DEBUG: "saving graph 9782435 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 5), (3, 6), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (7, 3), (8, 3), (7, 2), (8, 2)], 
step_length: 47, rl_cost: 164.0, reward: 38.599999999999945, str_out=max_reward=38.49999999999995 < ep_reward=38.599999999999945"
2018-04-24 20:34:05,223 (dqn_main.py:207) DEBUG: "saving graph 9789344 -> steps: 
[(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (5, 8), (6, 7), (6, 8), (7, 8), (8, 8), (8, 7), (7, 3), (8, 3), (7, 2), (8, 2)], 
step_length: 47, rl_cost: 162.0, reward: 38.69999999999995, str_out=max_reward=38.599999999999945 < ep_reward=38.69999999999995"
2018-04-24 20:57:07,374 (dqn_main.py:244) DEBUG: "Episode 210000, mean reward over last 10000 episodes: 15.573789999999976"
2018-04-24 20:57:07,374 (dqn_main.py:245) DEBUG: "Epsilon: 0.09999910000958534"
2018-04-24 20:57:07,374 (dqn_main.py:246) DEBUG: "RL steps: [(1, 2), (1, 3), (2, 2), (2, 3), (2, 4), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7), (2, 6), (2, 5), (3, 5), (3, 4), (3, 3), (3, 2), (4, 2), (4, 3), (4, 4), (4, 5), (5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (8, 5), (7, 5), (7, 4), (8, 4), (5, 3), (5, 2), (6, 2), (6, 3), (4, 6), (4, 7), (5, 7), (6, 7), (6, 8), (5, 8), (7, 8), (8, 8), (8, 7), (8, 3), (7, 3), (7, 2), (8, 2)], reward: 34.29999999999989, done: True"
2018-04-24 20:57:07,374 (dqn_main.py:247) DEBUG: "Steps: 46, coords: 48"
